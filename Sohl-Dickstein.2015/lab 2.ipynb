{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c696708",
   "metadata": {},
   "source": [
    "#### Things that I've accomplished so far: \n",
    "1) Worked through the paper to extent that I needed to\n",
    "2) Working prototype of obtaining the forward process\n",
    "\n",
    "#### Next step\n",
    "1) Fundamental working prototype of obtaining the loss for the reverse diffusion kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e65fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 10:50:04.807462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763405404.851891    5110 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763405404.864662    5110 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1763405404.971006    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763405404.971026    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763405404.971029    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1763405404.971032    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data with JAX / tensorflow_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 10:50:08.057170: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-11-17 10:50:08.213749: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-11-17 10:50:08.213882: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- JAX Data Info ---\n",
      "Data iterator created successfully.\n",
      "Batch of images shape: (128, 28, 28, 1)\n",
      "Data format: [Batch Size, Height, Width, Channels] (Channel-Last)\n",
      "Labels shape: (128,)\n",
      "Image data type: float32\n",
      "Image min value: -1.00\n",
      "Image max value: 1.00\n",
      "Label data type: int32\n"
     ]
    }
   ],
   "source": [
    "## imports, etc\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's just try something, why not? \n",
    "# I'm smart and quick on my feet, there's no need to be scared\n",
    "import tensorflow as tf  # Used for the data loading pipeline\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# I'll simplify this at some point, as it's pretty annoying, but it's\n",
    "# not super important right now\n",
    "def get_mnist_dataset(batch_size=64):\n",
    "    \"\"\"\n",
    "    Downloads and prepares the MNIST dataset using tensorflow_datasets,\n",
    "    and returns an iterator that yields JAX-compatible NumPy arrays.\n",
    "    \n",
    "    Returns:\n",
    "        An iterator that yields batches of (images, labels).\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Load the Dataset with TFDS ---\n",
    "    # as_supervised=True returns (image, label) tuples.\n",
    "    ds = tfds.load('mnist', split='train', as_supervised=True)\n",
    "    \n",
    "    # --- 2. Define the Preprocessing Function ---\n",
    "    # This function will be mapped over the dataset.\n",
    "    # TFDS loads images as uint8 [0, 255].\n",
    "    def normalize_img(image, label):\n",
    "        # Cast to float\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        # Normalize to [-1, 1]\n",
    "        # (pixel / 127.5) - 1.0\n",
    "        # (0 / 127.5) - 1.0 = -1.0\n",
    "        # (255 / 127.5) - 1.0 = 2.0 - 1.0 = 1.0\n",
    "        image = (image / 127.5) - 1.0\n",
    "        return image, label\n",
    "\n",
    "    # --- 3. Build the tf.data Pipeline ---\n",
    "    ds = ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.cache()  # Cache the preprocessed data in memory\n",
    "    ds = ds.shuffle(1000)  # Shuffle the dataset\n",
    "    ds = ds.batch(batch_size)  # Batch the data\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)  # Prefetch batches for performance\n",
    "    \n",
    "    # --- 4. Convert to NumPy Iterator ---\n",
    "    # tfds.as_numpy converts the tf.Tensor batches into NumPy arrays,\n",
    "    # which JAX can consume without any issues.\n",
    "    # ~ I am guessing where this step differs between JAX and Pytorch\n",
    "    return tfds.as_numpy(ds)\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # To run this, you'll need: \n",
    "    # pip install jax jaxlib tensorflow tensorflow-datasets\n",
    "    \n",
    "    # JAX models are pure functions, so the data pipeline is\n",
    "    # typically an iterator that yields NumPy arrays.\n",
    "    \n",
    "    print(\"Loading MNIST data with JAX / tensorflow_datasets...\")\n",
    "    train_iter = get_mnist_dataset(batch_size=128)\n",
    "    \n",
    "    # Get one batch of data\n",
    "    # 'images_np' will be a NumPy array of shape [batch_size, 28, 28, 1]\n",
    "    # 'labels_np' will be a NumPy array of shape [batch_size]\n",
    "    images_np, labels_np = next(iter(train_iter))\n",
    "    \n",
    "    # You can then convert them to JAX arrays for use in a train step\n",
    "    images_jnp = jnp.array(images_np)\n",
    "    labels_jnp = jnp.array(labels_np)\n",
    "    \n",
    "    print(f\"\\n--- JAX Data Info ---\")\n",
    "    print(f\"Data iterator created successfully.\")\n",
    "    print(f\"Batch of images shape: {images_jnp.shape}\")\n",
    "    print(f\"Data format: [Batch Size, Height, Width, Channels] (Channel-Last)\")\n",
    "    print(f\"Labels shape: {labels_jnp.shape}\")\n",
    "    print(f\"Image data type: {images_jnp.dtype}\")\n",
    "    print(f\"Image min value: {images_jnp.min():.2f}\")\n",
    "    print(f\"Image max value: {images_jnp.max():.2f}\")\n",
    "    print(f\"Label data type: {labels_jnp.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1454d2d",
   "metadata": {},
   "source": [
    "Idea: \n",
    "1) Let's take one image, and completely transform it to white Gaussian noise\n",
    "2) Try to compute the sigma and mu estimator for just one step---does this require having an MLP, CNN, etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d90ca",
   "metadata": {},
   "source": [
    "Looks like we'll need an MLP in order to actually implement the estimator, and therefore get a score off of how it will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e9eb4",
   "metadata": {},
   "source": [
    "Yes, indeed, they implement an MLP architecture which has hidden layers and convolutional layers. So, what I need to do is first figure out how to actually build conv layers in JAX, then I'll need to figure out how to build the MLP in JAX. Then, I'll circle back to actually implementing the version that's in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8928e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
