{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2deed72b",
   "metadata": {},
   "source": [
    "## Version 2, since now we're at a good point of re-factoring our code so that the BatchNorm layer is pretty clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849d1150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f60630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5a2b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182516, 3]),\n",
       " torch.Size([182516]),\n",
       " torch.Size([22815, 3]),\n",
       " torch.Size([22815]),\n",
       " torch.Size([22815, 3]),\n",
       " torch.Size([22815]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words, block_size):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    \n",
    "    # split up dataset\n",
    "    random.seed(42)\n",
    "    random.shuffle(words)\n",
    "    n1 = int(0.8*X.shape[0])\n",
    "    n2 = int(0.9*X.shape[0])\n",
    "    Xtr, Xdev, Xte = X.tensor_split((n1, n2), dim=0)\n",
    "    Ytr, Ydev, Yte = Y.tensor_split((n1, n2), dim=0)\n",
    "    \n",
    "    return Xtr, Ytr, Xdev, Ydev, Xte, Yte \n",
    "\n",
    "Xtr, Ytr, Xdev, Ydev, Xte, Yte = build_dataset(words, block_size)\n",
    "Xtr.shape, Ytr.shape, Xdev.shape, Ydev.shape, Xte.shape, Yte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcc37f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 10 \n",
    "n_hidden = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e8617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters: 12097\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn(vocab_size, n_emb, generator=g)\n",
    "W1 = torch.randn((block_size*n_emb, n_hidden), generator=g) * (5/3) / (block_size*n_emb)**0.5\n",
    "#b1 = torch.randn(n_hidden, generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0 # technically looking at this, we don't need it either\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(f'total parameters: {sum(p.nelement() for p in parameters)}')\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f3ebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 0 of 200000: 3.3127\n",
      "loss at step 10000 of 200000: 2.4849\n",
      "loss at step 20000 of 200000: 2.1814\n",
      "loss at step 30000 of 200000: 1.9586\n",
      "loss at step 40000 of 200000: 2.3361\n",
      "loss at step 50000 of 200000: 1.9597\n",
      "loss at step 60000 of 200000: 2.4565\n",
      "loss at step 70000 of 200000: 1.8806\n",
      "loss at step 80000 of 200000: 2.1031\n",
      "loss at step 90000 of 200000: 2.2282\n",
      "loss at step 100000 of 200000: 1.8709\n",
      "loss at step 110000 of 200000: 2.0971\n",
      "loss at step 120000 of 200000: 1.8410\n",
      "loss at step 130000 of 200000: 2.0784\n",
      "loss at step 140000 of 200000: 2.1063\n",
      "loss at step 150000 of 200000: 1.8091\n",
      "loss at step 160000 of 200000: 2.2664\n",
      "loss at step 170000 of 200000: 1.8477\n",
      "loss at step 180000 of 200000: 2.1799\n",
      "loss at step 190000 of 200000: 2.1433\n",
      "-----\n",
      "final loss: 1.974827186526749\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200_000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "epsilon = 1e-5 # additional variable which allows for more well behaved std when variance is 0\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "    ### mini-batch construction\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batches for X, Y\n",
    "    \n",
    "    ### forward pass\n",
    "    emb = C[Xb] # embed all characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate all vectors\n",
    "    \n",
    "    # linear layer\n",
    "    hpreact = embcat @ W1\n",
    "\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------------\n",
    "    bnmeani = hpreact.mean(dim=0, keepdim=True)\n",
    "    bnstdi = hpreact.std(dim=0, keepdim=True)    \n",
    "    hpreact = bngain*(hpreact - bnmeani)/(bnstdi + epsilon) + bnbias\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999*bnmean_running + 0.001*bnmeani\n",
    "        bnstd_running = 0.999*bnstd_running + 0.001*bnstdi\n",
    "    # -------------------------------------------------------------------\n",
    "    # non-linearity to construct outputs\n",
    "    h = torch.tanh(hpreact) # hidden layer construction\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "        \n",
    "    ### backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < int(max_steps/2) else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    if i % 10_000 == 0:\n",
    "        print(f'loss at step {i} of {max_steps}: {loss:.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "print(f'-----\\nfinal loss: {10**(lossi[-1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfadee",
   "metadata": {},
   "source": [
    "Number of features = number of neurons in our hidden layer\n",
    "\n",
    "The momentum is the scalar value that we use to update the running quantities by the in-situ calculated value obtained while we're training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e959d7",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e38c2b",
   "metadata": {},
   "source": [
    "The main point is that throughout our layer, we want Gaussian activations so that the behavior of the network is fairly stable regardless of our inputs. This will become critical when we begin to scale the number of layers in our MLP to do more complicated things\n",
    "\n",
    "Over time, BatchNorm got phased out because of all of these correlations that are introduced when the examples are coupled to each other when we are normalizing all of the batches during training. It causes errors and bugs and Andrej wants to spare us from this pain. There is stuff like LayerNorm which, I'm guessing just normalizes over an entire layer, but we aren't covering that yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b64830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're at an 1:20:00 roughly, and we're just starting to turn everything into modules so that it'll look just \n",
    "# like what we'd see when looking at PyTorch source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54f380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
