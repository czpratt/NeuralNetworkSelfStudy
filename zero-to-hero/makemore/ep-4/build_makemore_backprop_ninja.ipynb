{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rToK0Tku8PPn"
   },
   "source": [
    "## makemore: becoming a backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8sFElPqq8PPp"
   },
   "outputs": [],
   "source": [
    "# there no change change in the first several cells from last lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ChBbac4y8PPq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "x6GhEWW18aCS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-02 11:07:05--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 228145 (223K) [text/plain]\n",
      "Saving to: ‘names.txt.21’\n",
      "\n",
      "names.txt.21        100%[===================>] 222.80K  --.-KB/s    in 0.08s   \n",
      "\n",
      "2025-03-02 11:07:05 (2.86 MB/s) - ‘names.txt.21’ saved [228145/228145]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the names.txt file from github\n",
    "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "klmu3ZG08PPr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BCQomLE_8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V_zt2QHr8PPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eg20-vsg8PPt"
   },
   "outputs": [],
   "source": [
    "# ok biolerplate done, now we get to the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MJPU8HT08PPu"
   },
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZlFLjQyT8PPu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QY-y96Y48PPv"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) dlogprobs\n",
    "\n",
    "We need to obtain the derivative dloss / dlogprobs. This requires no chain rule since it's the very first gradient in the backpropogration.\n",
    "\n",
    "We need to unravel what the loss is doing: loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "logprobs[range(n), Yb] is forming a new tensor where each successive element corresponds to the element of Yb, i.e. logprobs[0][Yb[0]] + logprobs[1][Yb[1]] + ... + logprobs[31][Yb[31]] since n = 32. We divide this by 32. When taking the derivative of each element w.r.t logprobs, we're left with the normalization factor 1/32. Putting the negative back in we get -1/32. This also means that every element in logprobs that isn't directly associated with the loss has no gradient since it won't contribute to the output of loss\n",
    "\n",
    "What this means is that all elements in dlogprobs will be zero except for the ones that do directly contribute. Andrej gave the very simple example of this in his solution.\n",
    "\n",
    "loss = - (a + b + c) / 3 = -a/3 -b/3 -c/3, so for each element in logprobs this looks like -1/3 . In our case, 3->32 so every element should be -1/32 that isn't zero. \n",
    "\n",
    "we write this in one line as: `dlogprobs = torch.zeros_like(logprobs); dlogprobs[range(n), Yb] = -1.0/n`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) dprobs\n",
    "\n",
    "Now we have to start using the chain rule to backpropogate through the network\n",
    "\n",
    "dloss / dprobs = dloss / dlogprobs * dlogprobs / dprobs\n",
    "\n",
    "the first term on the rhs was just calculated, so all we need to do is calculate the second term. I'm going to use the derivative of the log for this one:\n",
    "\n",
    "dlogprobs / dprobs = d(log(probs)) / dprobs = 1 / probs\n",
    "\n",
    "This means I'm going to invert all of my elements in probs, and this completes this term.\n",
    "\n",
    "Now we have to figure out how to combine these together. We can either combine them using @ or *, which are matrix multiplication and Hadamard product (element wise multiplication) respectively\n",
    "\n",
    "Using @, this won't work since (dloss / dlogoprobs).shape == (dlogprobs / dprobs).shape = (32, 27). We can use tensor broadcasting for this though, where we just have to make sure that the dimensions are aligned properly\n",
    "\n",
    "(align from the right)\n",
    "\n",
    "(32, 27)\n",
    "(32, 27)\n",
    "\n",
    "and this will work! From: https://pytorch.org/docs/stable/notes/broadcasting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examples for broadcasting: \n",
    "# RULES: (1) each tensor must have dimension 1\n",
    "#        (2) starting at trailing dimension, each dimension must be equal, be 1, or DNE\n",
    "# final tensor will be the shape of the tensor with larger dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([[5, 5, 5], [6, 6, 6]])\n",
    "c = torch.tensor([[10], [20], [30]])\n",
    "d = torch.tensor([[5, 5], [6, 6], [7, 7], [8, 8]])\n",
    "e = torch.tensor([[9], [9], [9], [9], [9]])\n",
    "\n",
    "# a * b \n",
    "# (3) ====> (1, 3) Broadcastable ==> (2, 3) Final shape = (2, 3)\n",
    "# (2, 3) => (2, 3) ================> (2, 3)\n",
    "# a * b = [[1*5, 2*5, 3*5], [1*6, 2*6, 3*6]]\n",
    "\n",
    "# a * c\n",
    "# (3) ====> (1, 3) Broadcastable ==> (3, 3) Final shape = (3, 3) \n",
    "# (3, 1) => (3, 1) ================> (3, 1)\n",
    "# a * c = [[1*10, 2*10, 3*10], [1*20, 2*20, 3*20], [1*30, 2*30, 3*30]]\n",
    "\n",
    "# c * a\n",
    "# (3, 1) ==> (3, 1) Broadcastable ==> (3, 1)\n",
    "# (3) =====> (1, 3) ================> (3, 3)\n",
    "\n",
    "# d * a \n",
    "# (4, 2) => (4,   2) NOT broadcastable since 2 =/= 3\n",
    "# (3) ====> (DNE, 3)\n",
    "\n",
    "# a * d\n",
    "# (3) =====> (DNE, 3) NOT broadcastable since 3 =/= 2\n",
    "# (4, 2) ==> (4,   2)\n",
    "\n",
    "# b * e\n",
    "# (2, 3) NOT broadcastable since 2 =/= 5\n",
    "# (5, 1)\n",
    "\n",
    "# c * e\n",
    "# (3, 1) ==> NOT broadcastable since 3 =/= 5\n",
    "# (5, 1)\n",
    "# -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) dcounts_sum_inv\n",
    "\n",
    "dloss / dcsi = dloss / dprobs * dprobs / dcsi\n",
    "\n",
    "1st term on RHS we already got, but dprobs/dcsi needs to be expanded: d(counts * csi)/dcsi = counts\n",
    "\n",
    "this is the correct answer for the element-wise derivative. but we used csi multiple times (3) in order to broadcast the tensor properly. we see this after taking a look at counts * dprobs vs. shape of csi\n",
    "\n",
    "Remember that the derivative of whatever you were differentiating must have the same shape in order to properly account for the gradient. This means that dprobs * counts is definitely not the complete answer in this case. And it turns out it's because we have yet to account for the broadcasting that occured in order to obtain probs, which is counts * counts_sum_inv\n",
    "\n",
    "Remember from micrograd that when we were broadcasting vectors within our network, this amounted to adding up however many times this happened\n",
    "\n",
    "each csi element is the same variable that is used 32 times in order to propogate the gradient through the network, and these gradients summed together in the micrograd case. This would take the 27 appearences of the same variable within each \"use\" and sum it all together. We haven't done this summation trick yet, and we implement it by using (dprob * counts).sum(dim=1, keepdims=True)\n",
    "\n",
    "To back propogate through replication, we sum the gradients together within the rows---which gave the info. about how many times a column was utilized during a forward pass\n",
    "\n",
    "You can kind of think about the difference between \"how much did this one variable change the outcome each time it was replicated\", so you have to not only think about the element-wise multiplication, but also think about what happens to the gradients. It's almost like you're differentiating a vector function v = x(t), y(t), z(t)), which you do by: dv/dt = dx/dt xhat + dy/dt yhat + dz/dt zhat where because we've already normalized this vector function at this point, we already have unit vectors, which is why we add them all together. this is kind of like what's happening  in our case because we sum all of the gradients that are involved in this tensor broadcast, so really we're kind of looking at dprobs/dcsi = ([[dcounts_i/dcsi_i + ...], [...]).sum(dim=1, keepdim=True) so that we maintain the structure of the tensor so that it matches prob. \n",
    "\n",
    "There are multiple ways to think about it, but whenever we are using replication via tensor broadcasting, we must account for that within our back propogation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) dcounts_sum\n",
    "\n",
    "dloss / dcounts_sum = dloss / dcounts_sum_inv * dcounts_sum_inv / dcounts_sum\n",
    "\n",
    "A good rule of thumb: (1) always check the shapes of your tensors, so that you have an idea about what's going on\n",
    "\n",
    "dloss / dcounts_sum_inv we already have.\n",
    "\n",
    "The shapes of dcounts_sum_inv and counts_sum are the same, so there's no broadcasting, and therefore no replication of tensors happening in the backprop\n",
    "\n",
    "This means we can just treat this local derivative like a standard variable\n",
    "\n",
    "dcounts_sum_inv / dcounts_sum = d/dcounts_sum(counts_sum**-1) = -counts_sum**-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) dcounts\n",
    "\n",
    "counts is a little tricky since it was used twice---it's used in order to get probs, as well as counts_sum_inv, so there will be two contributions to its derivative due to being used twice. There will be a contribution from probs, as well as another contribution from counts_sum\n",
    "\n",
    "Andrej gives this to us, but it's found through:\n",
    "\n",
    "dloss/dcounts = dloss/dprobs * dprobs/dcounts + dloss/dcounts_sum * dcounts_sum/dcounts.\n",
    "\n",
    "First term is given * counts_sum_inv, and second term is below:\n",
    "\n",
    "for counts_sum, this is adding up all of the elements within each vector of counts. this is equivalent of the following case for the first row: \n",
    "\n",
    "counts = (a, b, c) ; counts.shape = (1, 3)\n",
    "d(a + b + c) / dcounts = d(a)/dcounts + d(b)/dcounts + d(c)/dcounts = (1, 0, 0) + (0, 1, 0) + (0, 0, 1) = (1,1,1)\n",
    "\n",
    "This is true because, for some function f and vector x = (x1, x2, x3), grad(f) = (df/dx1, df/dx2, df/dx3)\n",
    "\n",
    "This will be true for all vectors within counts, so this means we'll have an entire (32, 27) matrix of ones. And this checks out\n",
    "\n",
    "Andrej's explanation goes like: Addition is a router for the gradient to travel within the row vectors. Each row vector will have all 1's corresponding to the proper gradient, while all others will be zero. This will be true for each deriavative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm starting to get a general feel for what's going on and how to understand what's going on in the network: First check the shapes of the tensors you're working with. This will help you determine if there's some kind of broadcasting happening. Next is to create simple examples of what's happening, which will help you understand the fundamentals of a particular operation. Third is to scale up to the actual problem with both of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) dnorm_logits\n",
    "\n",
    "This one's easy since there's an exponential for the local derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) dlogit_maxes\n",
    "\n",
    "This one was trickier. The element-wise local derivative is all negative ones, since this is what we were subtracting from logits\n",
    "\n",
    "Also, the shapes are all over the place. logit_maxes.shape = [32, 1], dnorm_logits.shape = [32, 27] = logits.shape\n",
    "\n",
    "Also: `dlogit_maxes = dnorm_logits.sum(dim=1, keepdim=True)*(-torch.ones_like(logit_maxes))`\n",
    "\n",
    "can be just easily turned into: `-dlogit_maxes = -dnorm_logits.sum(dim=1, keepdim=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) dlogits\n",
    "\n",
    "I got this on the first try, and it was a two parter. I really feel like a ninja lmao\n",
    "\n",
    "The first term on RHS is simpler than the second; the local derivative will just be all ones since that's all what's relevant in norm_logits = logits - logit_maxes\n",
    "\n",
    "The second RHS term is trickier. We can use an easier example to understand what's going on\n",
    "\n",
    "let x = [[x11, x12], [x21, x22]] and xmax = [x11, x22] for example. Then when we take\n",
    "\n",
    "dxmax/dx = d/dx ([x11, x22]) = [[d/d(x11, x12)(x11), d/d(x11, x12)(x22)], [d/d(x21, x22)(x11), d/d(x21, x22)(x22)], this will create 1s wherever the maximum arg appears, but 0 everywhere else. Thinking in terms of coordinate vector spaces really helped me out here \n",
    "\n",
    "= [[d/d(x11, x12)(x11), d/d(x11, x12)(x22)], [d/d(x21, x22)(x11), d/d(x21, x22)(x22)]\n",
    "\n",
    "= [[((1, 0), (0, 0)), ((0, 0), (0, 1))] ==> [[1, 0], [0, 1]] after \"summing\" the most internal parentheses tuples ==> really it's representing only the non-zero \"phase space coordinates\" of the vectors this way\n",
    "\n",
    "This means that we just need to pluck out the indices of whereever the maximum element appears, and then set this to 1. I definitely had an inefficient way of doing this but I don't know how to optimize yet\n",
    "\n",
    "`imax = torch.tensor([torch.argmax(logits[i]) for i in range(n)])`\n",
    "\n",
    "`x = torch.zeros_like(logits); x[range(n), imax] = 1`\n",
    "\n",
    "AND THIS WORKED!\n",
    "\n",
    "And indeed, Andrej has a better solution\n",
    "\n",
    "using .max() also returns the indices! it's under logits.max(dim=1, keepdim=True).indices\n",
    "\n",
    "`# this gets the EXACT result, and was inspired by what we did previously!`\n",
    "\n",
    "`imax = torch.tensor([torch.argmax(logits[i]) for i in range(n)])`\n",
    "\n",
    "`x = torch.zeros_like(logits); x[range(n), imax] = 1`\n",
    "\n",
    "`# this gets the approximate result, but still clunky`\n",
    "\n",
    "`x = torch.zeros_like(logits); x[range(n), logits.max(dim=1, keepdim=True).indices] = 1`\n",
    "\n",
    "Also for the first term RHS, an equivalent of `dnorm_logits*torch.ones_like(logits)` is `torch.clone(dnorm_logits)`, which I'm assuming is more energy efficient\n",
    "\n",
    "The Andrej way is: `F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1])`, where the first argument is the indices where the maximum element occurs, and num_classes specifies the number of elements there will be in the row vector, so there will be a 1 wherever the max elem appeared in that index, and a 0 everywhere else in the row vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (9) dh\n",
    "\n",
    "Turns out I realize that I've never take the derivative of a matrix with respect to a matrix before, so this was an entirely new learning experience. It's much easier to first work out the details with a 2x2 matrix and then take the derivative of each element. For example consider the 2x2 matrix $\\mathbf{D} = \\mathbf{A}\\mathbf{B} + \\mathbf{C}$ and then work out the individual terms. The first element in the first row and column looks like\n",
    "\n",
    "$\\mathrm{d}\\mathbf{D}/\\mathrm{d}\\mathbf{A} = \\mathrm{d}d_{11}/\\mathrm{d}a_{11} + \\mathrm{d}d_{12}/\\mathrm{d}a_{11} = b_{11} \\mathrm{d}d_{11}/\\mathrm{d}a_{11} + b_{12} \\mathrm{d}d_{12}/\\mathrm{d}a_{11}$\n",
    "\n",
    "since the there are two terms which are dependent on $a_{11}$. Eventually what this looks like is:\n",
    "\n",
    "$\\mathrm{d}\\mathbf{D}/\\mathrm{d}\\mathbf{A}$ = $\\mathrm{d}\\mathbf{D}/\\mathrm{d}\\mathbf{A} @ B^T$\n",
    "\n",
    "and the final formula would look like:\n",
    "\n",
    "$\\mathrm{d}Loss/\\mathrm{d}h = \\mathrm{d}Loss / \\mathrm{d}logits @ \\mathbf{W2}^T$\n",
    "\n",
    "I'm not going to write out all of the other matrix elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (10) dW2\n",
    "\n",
    "Similar concept, except with Andrej's secret: Transpose the chain rule term, and then matrix multiply by dLoss/dlogits. Easy.\n",
    "\n",
    "#### (11)\n",
    "\n",
    "Same thing, need to eliminate the dimension so that the dimensions of the tensors match. So just sum up all of the rows of dlogits so that the dimensions match up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (12) dhpreact\n",
    "\n",
    "Pretty simple since $d(tanh(x))/dx = 1 - tanh^2(x)$ , and multiply this by the chain rule factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (13) dbngain\n",
    "\n",
    "Back to being tricky again. It first helps to look at the dimensions of things\n",
    "\n",
    "`dhpreact.shape = (32, 64), bngain.shape = (1, 64)`\n",
    "\n",
    "First we know by chain rule that there is a dhpreact and bnraw since every element contains an element wise multiplication of bngain and bnraw, which are linearly proportional. So the initial answer is bnraw * dhpreact. But now the dimensions don't add up--this is because bngain was broadcasted in the dim=0 dimension since you align dimensions on the right when you use tensor broadcasting. This means that we need to sum along this dimension in order to get the correct answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (14) dbnraw\n",
    "\n",
    "Same kind of thing here, but since the dimension of bnraw is (32, 64) you don't need to sum along any dimension since there was no broadcasting happening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (15) dbnbias\n",
    "\n",
    "Since bnbias is linear via addition in hpreact, we just need to sum along the appropriate dimension, that being dim=0 and keepdim=True as usual\n",
    "\n",
    "#### (16) dbnvar_inv\n",
    "\n",
    "Ditto to previous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (17) dbnvar \n",
    "\n",
    "Standard derivative taking with chain rule, nothing crazy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (18) dbndiff2\n",
    "\n",
    "A little tricky here with the sum along dim=0. dbnvar / dbndiff2 will give a torch.ones(bndiff2.shape) with elements being 32, but we are dividing by n-1, so no need to add it here since we're already having this normalizing part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (19)-(25)  bndiff - emb\n",
    "\n",
    "I did all of these in the span of 10-15 mins, I do feel like a backprop ninja now.\n",
    "\n",
    "Be wary of unbiased vs. bias variance calculations--better to stick with BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (26) C\n",
    "\n",
    "This was tricky forsure and it's also something I've never done before.\n",
    "\n",
    "The idea is that we want to pass in the gradients to each row of C, which corresponds to a 10-dimensional embedding of a character, depending on when it shows up in the batch. Xb tells us which row we should be depositing the gradient, and demb is the gradient that we need to pass through\n",
    "\n",
    "For example , `Xb[0] = tensor[1,1,4]` which tells us that we need to deposit gradients of demb corresponding to the first, first and fourth rows of demb into the first, first and fourth rows of dC. These gradients need to be additive as well, otherwise we won't capture the entire chain rule.\n",
    "\n",
    "Andrej says there's not a vectorized version of this so I'll take his word on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8ofj1s6d8PPv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3519, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(dim=1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mO-8aqxK8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bngain          | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
      "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.6088749766349792e-09\n",
      "emb             | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "C               | exact: False | approximate: True  | maxdiff: 6.05359673500061e-09\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# cross entropy loss\n",
    "dlogprobs = torch.zeros_like(logprobs); dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = dlogprobs * (1 / probs)\n",
    "dcounts_sum_inv = (dprobs * counts).sum(dim=1, keepdim=True)\n",
    "dcounts_sum = dcounts_sum_inv * (-counts_sum**(-2))\n",
    "dcounts = dprobs*counts_sum_inv + dcounts_sum*torch.ones_like(counts)\n",
    "dnorm_logits = dcounts*counts.clone()\n",
    "dlogit_maxes = -dnorm_logits.sum(dim=1, keepdim=True)\n",
    "dlogits = torch.clone(dnorm_logits) \n",
    "dlogits += F.one_hot(logits.max(dim=1).indices, num_classes=logits.shape[1])*dlogit_maxes\n",
    "\n",
    "# second linear layer\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(dim=0, keepdim=True)\n",
    "\n",
    "# non-linearity\n",
    "dhpreact = (1.0 - h.clone()**2) * dh\n",
    "\n",
    "# batch-norm layer\n",
    "dbngain = (bnraw * dhpreact).sum(dim=0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(dim=0, keepdim=True)\n",
    "dbnvar_inv = (dbnraw * bndiff).sum(dim=0, keepdim=True)\n",
    "dbnvar = dbnvar_inv * (-0.5 * (bnvar + 1e-5)**(-1.5))\n",
    "dbndiff2 = dbnvar * (1/(n-1)) * torch.ones(bndiff2.shape)\n",
    "dbndiff = (dbnraw * bnvar_inv) + (dbndiff2 * 2*bndiff)\n",
    "dbnmeani = -1.0 * dbndiff.sum(dim=0, keepdim=True)\n",
    "\n",
    "# first linear layer\n",
    "dhprebn = dbndiff + dbnmeani * (1/n)\n",
    "\n",
    "# forward pass\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(dim=0, keepdim=True)\n",
    "#demb = dembcat.reshape(emb.shape) # either one works\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for i, batch in enumerate(Xb):\n",
    "    for j, elem in enumerate(batch):\n",
    "        dC[elem.item()] += demb[i][j]\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7973, 1.0000, 0.1991, 0.4694, 0.2212, 0.9000, 0.2688, 0.3839, 0.2052,\n",
       "         0.3469, 0.3992, 0.4078, 0.3784, 0.2909, 0.4004, 0.1451, 0.1000, 0.2077,\n",
       "         0.1929, 0.5982, 0.5619, 0.2456, 0.2918, 0.7349, 0.6318, 0.2797, 0.2334],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([10.8917], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[0], counts_sum[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.shape, counts_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ebLtYji_8PPw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3132190704345703 diff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "-gCXbB4C8PPx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: False | maxdiff: 0.028187651187181473\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "## Completely misunderstood the asignment: :) --> (: --> :)\n",
    "''' ## Restarting below this but keeping this for now, getting the wrong answer anyway \n",
    "# -----------------\n",
    "# YOUR CODE HERE :) my solution is 3 lines\n",
    "t1 = -F.one_hot(logits.argmax(dim=1), num_classes=27)*torch.ones_like(logit_maxes) + torch.ones_like(logits)\n",
    "\n",
    "t2 = counts*(-1.0*torch.ones_like(counts)*counts_sum**-2 + counts_sum_inv)\n",
    "\n",
    "dlogits = -(1.0/n)*F.one_hot(Yb, num_classes=27)*(1/probs)*norm_logits.exp() * t1 * t2\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [32, 32] cannot be broadcast to indexing result of shape [32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ex \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(logits)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYb\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;241m*\u001b[39m counts_sum_inv \u001b[38;5;241m*\u001b[39m (counts_sum \u001b[38;5;241m-\u001b[39m counts[\u001b[38;5;28mrange\u001b[39m(n), Yb])\n\u001b[1;32m      4\u001b[0m ex\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [32, 32] cannot be broadcast to indexing result of shape [32]"
     ]
    }
   ],
   "source": [
    "ex = torch.zeros_like(logits)\n",
    "\n",
    "ex[range(n), Yb] = (-1/n) * counts_sum_inv * (counts_sum - counts[range(n), Yb])\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: False | maxdiff: 31.942655563354492\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :) my solution is 3 lines\n",
    "dlogits = torch.zeros_like(counts)\n",
    "dlogits[range(n), Yb] = -(n-1/n) \n",
    "\n",
    "# -----------------\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0714, 0.0870, 0.0177, 0.0502, 0.0194, 0.0835, 0.0224, 0.0350, 0.0188,\n",
       "         0.0324, 0.0339, 0.0367, 0.0385, 0.0292, 0.0363, 0.0138, 0.0088, 0.0198,\n",
       "         0.0164, 0.0585, 0.0489, 0.0209, 0.0250, 0.0686, 0.0612, 0.0249, 0.0208],\n",
       "        [0.0531, 0.0587, 0.0961, 0.0588, 0.0358, 0.0319, 0.0183, 0.0508, 0.0213,\n",
       "         0.0231, 0.0504, 0.0379, 0.0470, 0.0309, 0.0448, 0.0381, 0.0238, 0.0168,\n",
       "         0.0200, 0.0442, 0.0181, 0.0216, 0.0157, 0.0546, 0.0242, 0.0402, 0.0238],\n",
       "        [0.0202, 0.0250, 0.0147, 0.0148, 0.0237, 0.0404, 0.0533, 0.0629, 0.0623,\n",
       "         0.0316, 0.0195, 0.0326, 0.0453, 0.0517, 0.0246, 0.0267, 0.0141, 0.0347,\n",
       "         0.0280, 0.1128, 0.0652, 0.0329, 0.0387, 0.0378, 0.0394, 0.0194, 0.0277],\n",
       "        [0.0324, 0.0271, 0.0405, 0.0588, 0.0555, 0.0279, 0.0443, 0.0435, 0.0520,\n",
       "         0.0175, 0.0348, 0.0248, 0.0331, 0.0438, 0.0554, 0.0639, 0.0243, 0.0258,\n",
       "         0.0158, 0.0587, 0.0176, 0.0241, 0.0385, 0.0190, 0.0268, 0.0504, 0.0436],\n",
       "        [0.0169, 0.0162, 0.0243, 0.0243, 0.0186, 0.0370, 0.0325, 0.0190, 0.0579,\n",
       "         0.0387, 0.0305, 0.0137, 0.0345, 0.0244, 0.0490, 0.1727, 0.0193, 0.0531,\n",
       "         0.0477, 0.0642, 0.0414, 0.0155, 0.0215, 0.0134, 0.0444, 0.0231, 0.0462],\n",
       "        [0.0332, 0.0378, 0.0529, 0.0738, 0.0657, 0.0244, 0.0287, 0.0572, 0.0299,\n",
       "         0.0144, 0.0424, 0.0159, 0.0279, 0.0470, 0.0632, 0.0486, 0.0175, 0.0211,\n",
       "         0.0099, 0.0315, 0.0297, 0.0276, 0.0434, 0.0172, 0.0179, 0.0810, 0.0404],\n",
       "        [0.0524, 0.0123, 0.0695, 0.0163, 0.0291, 0.0194, 0.0647, 0.0543, 0.0506,\n",
       "         0.0424, 0.0168, 0.0321, 0.0255, 0.0193, 0.0161, 0.0747, 0.0802, 0.0448,\n",
       "         0.0106, 0.0351, 0.0262, 0.0474, 0.0419, 0.0205, 0.0424, 0.0257, 0.0298],\n",
       "        [0.0437, 0.0323, 0.0248, 0.1319, 0.0262, 0.0512, 0.0284, 0.0816, 0.0362,\n",
       "         0.0083, 0.0273, 0.0142, 0.0181, 0.0591, 0.0184, 0.0147, 0.0136, 0.0131,\n",
       "         0.0149, 0.0927, 0.0329, 0.0465, 0.0248, 0.0350, 0.0241, 0.0541, 0.0316],\n",
       "        [0.0485, 0.0315, 0.0182, 0.0370, 0.0104, 0.0434, 0.0505, 0.0851, 0.0296,\n",
       "         0.0202, 0.0278, 0.0295, 0.0363, 0.0239, 0.0124, 0.0196, 0.0190, 0.0415,\n",
       "         0.0237, 0.0988, 0.0346, 0.0775, 0.0415, 0.0454, 0.0471, 0.0158, 0.0310],\n",
       "        [0.0264, 0.0150, 0.0298, 0.0206, 0.0297, 0.0386, 0.0329, 0.0316, 0.0964,\n",
       "         0.0608, 0.0133, 0.0341, 0.0386, 0.0472, 0.0238, 0.0265, 0.0581, 0.0452,\n",
       "         0.0428, 0.0295, 0.0625, 0.0247, 0.0632, 0.0178, 0.0324, 0.0412, 0.0172],\n",
       "        [0.0168, 0.0082, 0.0393, 0.0443, 0.0327, 0.0360, 0.0167, 0.0066, 0.0408,\n",
       "         0.0452, 0.0244, 0.0182, 0.0536, 0.0303, 0.0390, 0.1452, 0.0517, 0.0568,\n",
       "         0.0781, 0.0381, 0.0458, 0.0096, 0.0121, 0.0219, 0.0369, 0.0283, 0.0236],\n",
       "        [0.0230, 0.0312, 0.0212, 0.1963, 0.0117, 0.0563, 0.0609, 0.0468, 0.0426,\n",
       "         0.0304, 0.0146, 0.0179, 0.0212, 0.0279, 0.0067, 0.0068, 0.0203, 0.0345,\n",
       "         0.0174, 0.0591, 0.0511, 0.0391, 0.0186, 0.0367, 0.0474, 0.0455, 0.0147],\n",
       "        [0.0184, 0.0143, 0.0484, 0.0156, 0.0268, 0.0153, 0.0794, 0.0401, 0.0578,\n",
       "         0.0547, 0.0167, 0.0352, 0.0360, 0.0313, 0.0155, 0.0406, 0.0742, 0.0491,\n",
       "         0.0429, 0.0257, 0.0447, 0.0358, 0.0931, 0.0101, 0.0227, 0.0277, 0.0280],\n",
       "        [0.0234, 0.0292, 0.0582, 0.0226, 0.0167, 0.0696, 0.0256, 0.0170, 0.0317,\n",
       "         0.0430, 0.0262, 0.0302, 0.0660, 0.0380, 0.0492, 0.0467, 0.0312, 0.0351,\n",
       "         0.0840, 0.0164, 0.0628, 0.0150, 0.0174, 0.0339, 0.0361, 0.0395, 0.0355],\n",
       "        [0.0484, 0.0264, 0.0281, 0.0569, 0.0167, 0.0344, 0.0306, 0.0598, 0.0536,\n",
       "         0.0476, 0.0308, 0.0409, 0.0508, 0.0400, 0.0278, 0.0235, 0.0200, 0.0593,\n",
       "         0.0279, 0.0546, 0.0509, 0.0451, 0.0276, 0.0317, 0.0228, 0.0227, 0.0210],\n",
       "        [0.0445, 0.0620, 0.0340, 0.0196, 0.0183, 0.0794, 0.0302, 0.0238, 0.0288,\n",
       "         0.0498, 0.0319, 0.0491, 0.0631, 0.0171, 0.0271, 0.0170, 0.0417, 0.0196,\n",
       "         0.0606, 0.0192, 0.0377, 0.0262, 0.0317, 0.0619, 0.0575, 0.0291, 0.0193],\n",
       "        [0.0277, 0.0260, 0.0851, 0.0479, 0.0575, 0.0470, 0.0165, 0.0285, 0.0216,\n",
       "         0.0180, 0.0475, 0.0208, 0.0391, 0.0613, 0.0691, 0.0606, 0.0149, 0.0179,\n",
       "         0.0235, 0.0495, 0.0268, 0.0168, 0.0100, 0.0425, 0.0232, 0.0484, 0.0525],\n",
       "        [0.0486, 0.0844, 0.0482, 0.0559, 0.0364, 0.0334, 0.0242, 0.0753, 0.0263,\n",
       "         0.0162, 0.0659, 0.0139, 0.0230, 0.0183, 0.0403, 0.0289, 0.0149, 0.0319,\n",
       "         0.0170, 0.0604, 0.0203, 0.0422, 0.0260, 0.0178, 0.0254, 0.0434, 0.0616],\n",
       "        [0.0184, 0.0143, 0.0484, 0.0156, 0.0268, 0.0153, 0.0794, 0.0401, 0.0578,\n",
       "         0.0547, 0.0167, 0.0352, 0.0360, 0.0313, 0.0155, 0.0406, 0.0742, 0.0491,\n",
       "         0.0429, 0.0257, 0.0447, 0.0358, 0.0931, 0.0101, 0.0227, 0.0277, 0.0280],\n",
       "        [0.0245, 0.0235, 0.0361, 0.0175, 0.0393, 0.0335, 0.0629, 0.0503, 0.0348,\n",
       "         0.0375, 0.0287, 0.0442, 0.0570, 0.0575, 0.0141, 0.0239, 0.0299, 0.0199,\n",
       "         0.0304, 0.0921, 0.0494, 0.0285, 0.0380, 0.0457, 0.0336, 0.0222, 0.0251],\n",
       "        [0.0565, 0.0144, 0.0182, 0.0449, 0.0303, 0.0359, 0.0266, 0.0393, 0.0599,\n",
       "         0.0224, 0.0617, 0.0358, 0.0555, 0.0234, 0.0439, 0.0717, 0.0157, 0.0217,\n",
       "         0.0333, 0.0563, 0.0196, 0.0292, 0.0232, 0.0233, 0.0368, 0.0621, 0.0383],\n",
       "        [0.0535, 0.0742, 0.0478, 0.0533, 0.0277, 0.0559, 0.0287, 0.0482, 0.0252,\n",
       "         0.0161, 0.0501, 0.0245, 0.0218, 0.0344, 0.0416, 0.0419, 0.0184, 0.0360,\n",
       "         0.0201, 0.0939, 0.0132, 0.0301, 0.0144, 0.0290, 0.0369, 0.0311, 0.0319],\n",
       "        [0.0184, 0.0143, 0.0484, 0.0156, 0.0268, 0.0153, 0.0794, 0.0401, 0.0578,\n",
       "         0.0547, 0.0167, 0.0352, 0.0360, 0.0313, 0.0155, 0.0406, 0.0742, 0.0491,\n",
       "         0.0429, 0.0257, 0.0447, 0.0358, 0.0931, 0.0101, 0.0227, 0.0277, 0.0280],\n",
       "        [0.0464, 0.0191, 0.0266, 0.0136, 0.0549, 0.0257, 0.0604, 0.0344, 0.0240,\n",
       "         0.0439, 0.0312, 0.0360, 0.0343, 0.0372, 0.0365, 0.0456, 0.0731, 0.0224,\n",
       "         0.0366, 0.0170, 0.0164, 0.0228, 0.1147, 0.0162, 0.0608, 0.0340, 0.0163],\n",
       "        [0.0324, 0.0797, 0.0230, 0.0503, 0.0273, 0.0616, 0.0228, 0.0335, 0.0277,\n",
       "         0.0278, 0.1011, 0.0150, 0.0288, 0.0129, 0.0376, 0.0193, 0.0164, 0.0153,\n",
       "         0.0124, 0.0330, 0.0135, 0.0223, 0.0110, 0.1441, 0.0526, 0.0275, 0.0513],\n",
       "        [0.0655, 0.0297, 0.0148, 0.0211, 0.0284, 0.0855, 0.0405, 0.0313, 0.0275,\n",
       "         0.0357, 0.0301, 0.0478, 0.0556, 0.0364, 0.0207, 0.0128, 0.0315, 0.0217,\n",
       "         0.0545, 0.0157, 0.0236, 0.0213, 0.0682, 0.0494, 0.0727, 0.0477, 0.0101],\n",
       "        [0.0413, 0.0273, 0.0220, 0.0114, 0.0460, 0.0219, 0.0813, 0.0436, 0.0277,\n",
       "         0.0516, 0.0176, 0.0445, 0.0568, 0.0333, 0.0250, 0.0182, 0.0592, 0.0260,\n",
       "         0.0298, 0.0377, 0.0201, 0.0349, 0.1255, 0.0171, 0.0422, 0.0286, 0.0097],\n",
       "        [0.0267, 0.0290, 0.0536, 0.0215, 0.0332, 0.0208, 0.0359, 0.0321, 0.0416,\n",
       "         0.0513, 0.0200, 0.0276, 0.0357, 0.0395, 0.0436, 0.0636, 0.0478, 0.0474,\n",
       "         0.0495, 0.0190, 0.0918, 0.0141, 0.0455, 0.0108, 0.0167, 0.0359, 0.0459],\n",
       "        [0.0670, 0.0425, 0.0166, 0.0219, 0.0323, 0.0627, 0.0382, 0.0394, 0.0100,\n",
       "         0.0253, 0.0415, 0.0264, 0.0243, 0.0233, 0.0260, 0.0310, 0.0762, 0.0243,\n",
       "         0.0681, 0.0272, 0.0321, 0.0299, 0.0427, 0.0569, 0.0832, 0.0127, 0.0183],\n",
       "        [0.0214, 0.0187, 0.0331, 0.0355, 0.0472, 0.0222, 0.0607, 0.0345, 0.0758,\n",
       "         0.0326, 0.0264, 0.0354, 0.0304, 0.0390, 0.0296, 0.0645, 0.0308, 0.0275,\n",
       "         0.0365, 0.0349, 0.0391, 0.0272, 0.0572, 0.0124, 0.0339, 0.0580, 0.0358],\n",
       "        [0.0399, 0.0548, 0.0247, 0.0664, 0.0466, 0.0488, 0.0298, 0.0320, 0.0257,\n",
       "         0.0198, 0.0352, 0.0203, 0.0157, 0.0270, 0.0110, 0.0328, 0.0364, 0.0276,\n",
       "         0.0457, 0.0548, 0.0533, 0.0253, 0.0201, 0.0719, 0.0873, 0.0214, 0.0257],\n",
       "        [0.0440, 0.0250, 0.0703, 0.0204, 0.0667, 0.0261, 0.0165, 0.0259, 0.0159,\n",
       "         0.0530, 0.0283, 0.0285, 0.0335, 0.0905, 0.0401, 0.0382, 0.0482, 0.0428,\n",
       "         0.0423, 0.0158, 0.0522, 0.0100, 0.0652, 0.0207, 0.0197, 0.0412, 0.0190]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "hd-MkhB68PPy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POdeZSKT8PPy"
   },
   "outputs": [],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = None # TODO. my solution is 1 (long) line\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPy8DhqB8PPz"
   },
   "outputs": [],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "#with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmean = hprebn.mean(0, keepdim=True)\n",
    "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "  hpreact = bngain * bnraw + bnbias\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "  # manual backprop! #swole_doge_meme\n",
    "  # -----------------\n",
    "  # YOUR CODE HERE :)\n",
    "  dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
    "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "  # -----------------\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p, grad in zip(parameters, grads):\n",
    "    p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "    #p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "  if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEpI0hMW8PPz"
   },
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KImLWNoh8PP0"
   },
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aFnP_Zc8PP0"
   },
   "outputs": [],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esWqmhyj8PP1"
   },
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xHeQNv3s8PP1"
   },
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "\n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
