{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f41675",
   "metadata": {},
   "source": [
    "Part 4, because now we're going to turn this into a full scale training loop, and i think getting to this step is super bad ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "688562ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e401d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4b1d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "print(max(itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9320b8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs: tensor([ 0,  5, 13,  ..., 25, 26, 24])\n",
      "xs.size: 228146\n",
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# Build your datasets\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "# then we'll turn these into tensors since we'll use them to build the network with PyTorch\n",
    "xs = torch.tensor(xs)\n",
    "print(f'xs: {xs}')\n",
    "print(f'xs.size: {xs.size()[0]}')\n",
    "ys = torch.tensor(ys)\n",
    "print(f'number of examples: {xs.nelement()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b95fc8",
   "metadata": {},
   "source": [
    "Now instead of using one_hot to pick out the row of W, I basically just want to delete this, and obtain it some other way. This will give insight into how to make my code better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "637ddb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 1 of 10: 3.759\n",
      "loss at step 2 of 10: 3.371\n",
      "loss at step 3 of 10: 3.154\n",
      "loss at step 4 of 10: 3.02\n",
      "loss at step 5 of 10: 2.928\n",
      "loss at step 6 of 10: 2.86\n",
      "loss at step 7 of 10: 2.81\n",
      "loss at step 8 of 10: 2.77\n",
      "loss at step 9 of 10: 2.738\n",
      "loss at step 10 of 10: 2.711\n"
     ]
    }
   ],
   "source": [
    "# Let's build our training loop\n",
    "\n",
    "# initialize your random number generator from which you will obtain weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "\n",
    "train_steps = 10\n",
    "logits = torch.empty(xs.size()[0], W.size()[0])\n",
    "#print(f'logits: {logits}')\n",
    "for k in range(train_steps):\n",
    "    ### Forward pass ###\n",
    "    # instead of using one_hot, remember that we can index W using a tensor\n",
    "    # but since xs is just a tensor full of indices, we can use these to pick out\n",
    "    # the particular matrix which corresponds to an element of W\n",
    "    # that is associated with the particular character        \n",
    "    logits = W[xs]\n",
    "    \n",
    "    #counts = logits.exp()\n",
    "    #probs = counts / counts.sum(1, keepdims=True)\n",
    "    #loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None # remember to always zero your grads\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update data\n",
    "    W.data += -50 * W.grad\n",
    "    print(f'loss at step {k+1} of {train_steps}: {loss.item():.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f83ff",
   "metadata": {},
   "source": [
    "We are supposed to be able to approach the original loss value that we got when we were just counting how many times a character would appear in the dataset afte smoothing, this was about 2.47, which is exactly what I'm getting now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cfce6",
   "metadata": {},
   "source": [
    "There's this awesome concept called regularization, which we can add onto our gradient descent algorithm to train our network. It's equivalent to making the smoothing in the `counting' scenario go from N+1 -> N+1e5, since all  this does is make all of the probabilities go to the uniform distribution\n",
    "\n",
    "What we are after now is to be able to try to push all of the weights of W to go to zero. What this does is it sends the logits ==> exp.() to 1, so that the probabilities end up becoming the uniform distribution, and thus completely matching the uniform case for the counting scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4146a209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 1 of 10: 3.779\n",
      "loss at step 2 of 10: 0.2941\n",
      "loss at step 3 of 10: 0.3156\n",
      "loss at step 4 of 10: 1.172\n",
      "loss at step 5 of 10: 2.84\n",
      "loss at step 6 of 10: 1.199\n",
      "loss at step 7 of 10: 2.816\n",
      "loss at step 8 of 10: 1.223\n",
      "loss at step 9 of 10: 2.794\n",
      "loss at step 10 of 10: 1.244\n"
     ]
    }
   ],
   "source": [
    "# Let's build our training loop\n",
    "\n",
    "# initialize your random number generator from which you will obtain weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "\n",
    "train_steps = 10\n",
    "for k in range(train_steps):\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None # remember to always zero your grads\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update data\n",
    "    W.data += -50 * W.grad\n",
    "    print(f'loss at step {k+1} of {train_steps}: {loss.item():.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920801e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ce9f8d5",
   "metadata": {},
   "source": [
    "And now, let's sample from the neural network and see what kind of results we get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37b7a5ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m xenc \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(torch\u001b[38;5;241m.\u001b[39mtensor([ix]), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m27\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      8\u001b[0m logits \u001b[38;5;241m=\u001b[39m xenc \u001b[38;5;241m@\u001b[39m W\n\u001b[0;32m----> 9\u001b[0m counts \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m probs \u001b[38;5;241m=\u001b[39m counts \u001b[38;5;241m/\u001b[39m counts\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, replacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, generator\u001b[38;5;241m=\u001b[39mg)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bafb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
