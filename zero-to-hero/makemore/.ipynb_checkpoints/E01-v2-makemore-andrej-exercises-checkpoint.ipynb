{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110629cb",
   "metadata": {},
   "source": [
    "**E01:** train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fdc661",
   "metadata": {},
   "source": [
    "### First step: How do I train a trigram model? First I need to start with trying to understand how to actually produce tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d194bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15607744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ca13c",
   "metadata": {},
   "source": [
    "Now we need to set up chars so that stoi and itos are configured properly. We should have to try to identify the next letter, so I'm not quite sure how I'll figure out how to do this yet\n",
    "\n",
    "I'm thinking we'll just need two sets of dictionaries in order to properly map everything together, since we want all of the trigrams to be referenced to unique sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12b03afc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "single_chr = sorted(list(set(''.join(words))))\n",
    "single_stoi = {s:i+1 for i, s in enumerate(single_chr)} # reserve elem 0 for the dot\n",
    "single_stoi['.'] = 0\n",
    "single_itos = {i:s for s, i in single_stoi.items()} # flip integers and strings around\n",
    "double_chr = [f'{ch1}{ch2}' for ch1 in single_itos.values() for ch2 in single_itos.values()]\n",
    "double_stoi = {s:i for i, s in enumerate(double_chr)}\n",
    "del double_stoi['..']\n",
    "double_itos = {i:s for s, i in double_stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b4f663d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 196113\n"
     ]
    }
   ],
   "source": [
    "# Build your datasets\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    #print(chs)\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch12 = ch1 + ch2\n",
    "        ix_d = double_stoi[ch12] # double character index\n",
    "        ix_s = single_stoi[ch3]  # single character index\n",
    "        #print(f'ch12 ch3: {ch12} {ch3}')\n",
    "        #print(f'ix_d, ix_s: {ix_d}, {ix_s}')\n",
    "        xs.append(ix_d)\n",
    "        ys.append(ix_s)\n",
    "        \n",
    "# then we'll turn these into tensors since we'll use them to build the network with PyTorch\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f'number of examples: {xs.nelement()}')\n",
    "#print(f'xs: {xs}')\n",
    "#print(f'ys: {ys}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1fa6422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg neg log likelihood loss in step 1 of 50: 3.7883\n",
      "avg neg log likelihood loss in step 2 of 50: 3.7083\n",
      "avg neg log likelihood loss in step 3 of 50: 3.6333\n",
      "avg neg log likelihood loss in step 4 of 50: 3.5629\n",
      "avg neg log likelihood loss in step 5 of 50: 3.4971\n",
      "avg neg log likelihood loss in step 6 of 50: 3.4358\n",
      "avg neg log likelihood loss in step 7 of 50: 3.3790\n",
      "avg neg log likelihood loss in step 8 of 50: 3.3265\n",
      "avg neg log likelihood loss in step 9 of 50: 3.2782\n",
      "avg neg log likelihood loss in step 10 of 50: 3.2339\n",
      "avg neg log likelihood loss in step 11 of 50: 3.1933\n",
      "avg neg log likelihood loss in step 12 of 50: 3.1560\n",
      "avg neg log likelihood loss in step 13 of 50: 3.1217\n",
      "avg neg log likelihood loss in step 14 of 50: 3.0899\n",
      "avg neg log likelihood loss in step 15 of 50: 3.0605\n",
      "avg neg log likelihood loss in step 16 of 50: 3.0330\n",
      "avg neg log likelihood loss in step 17 of 50: 3.0074\n",
      "avg neg log likelihood loss in step 18 of 50: 2.9833\n",
      "avg neg log likelihood loss in step 19 of 50: 2.9606\n",
      "avg neg log likelihood loss in step 20 of 50: 2.9392\n",
      "avg neg log likelihood loss in step 21 of 50: 2.9189\n",
      "avg neg log likelihood loss in step 22 of 50: 2.8997\n",
      "avg neg log likelihood loss in step 23 of 50: 2.8815\n",
      "avg neg log likelihood loss in step 24 of 50: 2.8642\n",
      "avg neg log likelihood loss in step 25 of 50: 2.8477\n",
      "avg neg log likelihood loss in step 26 of 50: 2.8319\n",
      "avg neg log likelihood loss in step 27 of 50: 2.8169\n",
      "avg neg log likelihood loss in step 28 of 50: 2.8025\n",
      "avg neg log likelihood loss in step 29 of 50: 2.7887\n",
      "avg neg log likelihood loss in step 30 of 50: 2.7754\n",
      "avg neg log likelihood loss in step 31 of 50: 2.7628\n",
      "avg neg log likelihood loss in step 32 of 50: 2.7506\n",
      "avg neg log likelihood loss in step 33 of 50: 2.7389\n",
      "avg neg log likelihood loss in step 34 of 50: 2.7276\n",
      "avg neg log likelihood loss in step 35 of 50: 2.7168\n",
      "avg neg log likelihood loss in step 36 of 50: 2.7063\n",
      "avg neg log likelihood loss in step 37 of 50: 2.6962\n",
      "avg neg log likelihood loss in step 38 of 50: 2.6865\n",
      "avg neg log likelihood loss in step 39 of 50: 2.6772\n",
      "avg neg log likelihood loss in step 40 of 50: 2.6681\n",
      "avg neg log likelihood loss in step 41 of 50: 2.6594\n",
      "avg neg log likelihood loss in step 42 of 50: 2.6509\n",
      "avg neg log likelihood loss in step 43 of 50: 2.6427\n",
      "avg neg log likelihood loss in step 44 of 50: 2.6348\n",
      "avg neg log likelihood loss in step 45 of 50: 2.6272\n",
      "avg neg log likelihood loss in step 46 of 50: 2.6197\n",
      "avg neg log likelihood loss in step 47 of 50: 2.6125\n",
      "avg neg log likelihood loss in step 48 of 50: 2.6055\n",
      "avg neg log likelihood loss in step 49 of 50: 2.5987\n",
      "avg neg log likelihood loss in step 50 of 50: 2.5921\n",
      "tensor(2.5921, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_steps = 50\n",
    "train_step = 50\n",
    "num_inputs = 27*27-1\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((num_inputs, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "for k in range(train_steps):\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=num_inputs).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    anll = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None\n",
    "    anll.backward()\n",
    "    \n",
    "    # Update\n",
    "    W.data += -train_step * W.grad\n",
    "    print(f'avg neg log likelihood loss in step {k+1} of {train_steps}: {anll:.4f}')\n",
    "    \n",
    "print(anll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a853c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouwjdldjawcqid.\n",
      "pxlfbywednw.\n",
      ".\n",
      "oh.\n",
      "nwtozshsh.\n",
      "g.\n",
      "zvbmaxnpauydbbleviajsdbyuinrwipblasnjyinbyt.\n",
      "rtbcffrmumtsyfodtumjmnpytszwjqrsaed.\n",
      "roreayg.\n",
      "zpcejajaaedlwtdfmiiibwyfinwtg.\n",
      "psnhsvfihsuszphddg.\n",
      "nfbptpariluir.\n",
      "paufbtkit.\n",
      "r.\n",
      "nbmri.\n",
      "isuyuytr.\n",
      "nmeaujibkivuywtdlpch.\n",
      ".\n",
      "ywpg.\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(20):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=num_inputs).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(single_itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb343527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
