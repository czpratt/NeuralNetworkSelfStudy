{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00db815f",
   "metadata": {},
   "source": [
    "### E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b04a0",
   "metadata": {},
   "source": [
    "#### Let's do the bigram model first and see how we can be able to split the dataset up into different sets, and then use them for different purposes\n",
    "\n",
    "I have an idea\n",
    "\n",
    "I think what I need to do is create the dataset W, and then recreate my xs so that I can then use it to evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d1dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "words = open('names.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b860aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 182592\n",
      "xenc: tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "xenc.shape: torch.Size([182592, 27])\n",
      "logits: tensor([[ 1.5674, -0.2373, -0.0274,  ..., -0.0707,  2.4968,  2.4448],\n",
      "        [-0.5913, -0.1106,  0.8342,  ...,  0.4161,  0.6849, -0.1479],\n",
      "        [-0.7786,  1.2910, -1.5094,  ..., -0.8067, -0.2417,  1.5490],\n",
      "        ...,\n",
      "        [ 0.4724,  1.4830,  0.3175,  ..., -0.4275, -2.1259,  0.9604],\n",
      "        [ 0.7414, -0.5879, -0.4651,  ..., -0.1388,  1.3096, -0.2580],\n",
      "        [-0.6701, -1.2199,  0.3031,  ...,  0.8032,  0.5411, -1.1646]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "loss at step 1 of 1: 3.771\n",
      "training done. avg loss: 3.7708921432495117\n"
     ]
    }
   ],
   "source": [
    "# set up the look up tables that will be used for training the neural network\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "# Now we need to split up the dataset\n",
    "random.shuffle(words)\n",
    "split_80 = int(0.8 * len(words))\n",
    "split_90 = int(0.9 * len(words))\n",
    "\n",
    "train_dataset = words[:split_80]\n",
    "dev_dataset = words[split_80:split_90]\n",
    "test_dataset = words[split_90:]\n",
    "\n",
    "# Build the inputs and target labels\n",
    "xs, ys = [], []\n",
    "for w in train_dataset:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "# convert inputs and target labels as torch.tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f'number of examples: {xs.nelement()}')\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "train_steps = 1\n",
    "for k in range(train_steps):\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    print(f'xenc: {xenc}')\n",
    "    print(f'xenc.shape: {xenc.shape}')\n",
    "    logits = xenc @ W\n",
    "    print(f'logits: {logits}')\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "\n",
    "    # Backward pass\n",
    "    W.grad = None # remember to always zero your grads\n",
    "    loss.backward()\n",
    "\n",
    "    # Update data\n",
    "    W.data += -50 * W.grad\n",
    "    if k % 10 == 0:\n",
    "        print(f'loss at step {k+1} of {train_steps}: {loss.item():.4}')     \n",
    "print(f'training done. avg loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910423a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 22871\n"
     ]
    }
   ],
   "source": [
    "# maybe what Andrej means is that I create a different dataset, \n",
    "# based on the words in my test set\n",
    "xs, ys = [], []\n",
    "for w in test_dataset:\n",
    "    #print(f'word: {w}')\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "# convert inputs and target labels as torch.tensors\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f'number of examples: {xs.nelement()}')\n",
    "#print(f'xs: {xs}')\n",
    "#print(f'ys: {ys}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3e399b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now printing 10 names based on our weights\n",
      "-----\n",
      "h.\n",
      "taytryaabane.\n",
      "asadona.\n",
      ".\n",
      "dnhq.\n",
      "s.\n",
      "lelpfhableelsaa.\n",
      "ezge.\n",
      "eavdi.\n",
      "emagejh.\n"
     ]
    }
   ],
   "source": [
    "# so now what we're going to do is rebuild the test set, and then use it to predict our names\n",
    "num_names = 10\n",
    "print(f'now printing {num_names} names based on our weights')\n",
    "print('-----')\n",
    "\n",
    "# my current plan is that i am iterating through xs, and trying to just\n",
    "# predict whatever will come next with my classifier\n",
    "index = 0\n",
    "for i in range(num_names):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        # encoded input will be input from xs directly\n",
    "        xin = xs[index].reshape(1)\n",
    "        xenc = F.one_hot(xin, num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        # the index will be chosen randomly from probability distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "        index += 1\n",
    "        if index == len(xs) - 1:\n",
    "            break\n",
    "        \n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77337dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59aff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
