{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00db815f",
   "metadata": {},
   "source": [
    "### E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b04a0",
   "metadata": {},
   "source": [
    "#### Let's do the bigram model first and see how we can be able to split the dataset up into different sets, and then use them for different purposes\n",
    "\n",
    "## I'm stuck on how to actually \"evaluate the models\" on the dev and test splits. What does this even mean? I need to figure this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14d1dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "words = open('names.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51e3c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_classifier_with_split_datasets():\n",
    "    # set up the look up tables that will be used for training the neural network\n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "    stoi['.'] = 0\n",
    "    itos = {i:s for s, i in stoi.items()}\n",
    "    \n",
    "    # Now we need to split up the dataset\n",
    "    random.shuffle(words)\n",
    "    split_80 = int(0.8 * len(words))\n",
    "    split_90 = int(0.9 * len(words))\n",
    "    \n",
    "    train_dataset = words[:split_80]\n",
    "    dev_dataset = words[split_80:split_90]\n",
    "    test_dataset = words[split_90:]\n",
    "    \n",
    "    # Build the inputs and target labels\n",
    "    xs, ys = [], []\n",
    "    for w in train_dataset:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            xs.append(ix1)\n",
    "            ys.append(ix2)\n",
    "            \n",
    "    # convert inputs and target labels as torch.tensors\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    print(f'number of examples: {xs.nelement()}')\n",
    "\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((27, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "    train_steps = 50\n",
    "    for k in range(train_steps):\n",
    "        # Forward pass\n",
    "        xenc = F.one_hot(xs, num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "\n",
    "        # Backward pass\n",
    "        W.grad = None # remember to always zero your grads\n",
    "        loss.backward()\n",
    "\n",
    "        # Update data\n",
    "        W.data += -50 * W.grad\n",
    "        if k % 10 == 0:\n",
    "            print(f'loss at step {k+1} of {train_steps}: {loss.item():.4}')\n",
    "        \n",
    "    print(f'loss at the end of training: {loss}')\n",
    "    print('-----')\n",
    "    num_names = 10\n",
    "    print(f'now printing {num_names} names based on our weights')\n",
    "    print('-----')\n",
    "    for i in range(num_names):\n",
    "        out = []\n",
    "        ix = 0\n",
    "        while True:\n",
    "            xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "            logits = xenc @ W\n",
    "            counts = logits.exp()\n",
    "            probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "            ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "            out.append(itos[ix])\n",
    "            if ix == 0:\n",
    "                break\n",
    "\n",
    "        print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5524133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 182289\n",
      "loss at step 1 of 50: 3.769\n",
      "loss at step 2 of 50: 3.379\n",
      "loss at step 3 of 50: 3.161\n",
      "loss at step 4 of 50: 3.027\n",
      "loss at step 5 of 50: 2.934\n",
      "loss at step 6 of 50: 2.867\n",
      "loss at step 7 of 50: 2.816\n",
      "loss at step 8 of 50: 2.777\n",
      "loss at step 9 of 50: 2.745\n",
      "loss at step 10 of 50: 2.718\n",
      "loss at step 11 of 50: 2.696\n",
      "loss at step 12 of 50: 2.677\n",
      "loss at step 13 of 50: 2.66\n",
      "loss at step 14 of 50: 2.646\n",
      "loss at step 15 of 50: 2.633\n",
      "loss at step 16 of 50: 2.622\n",
      "loss at step 17 of 50: 2.612\n",
      "loss at step 18 of 50: 2.603\n",
      "loss at step 19 of 50: 2.595\n",
      "loss at step 20 of 50: 2.588\n",
      "loss at step 21 of 50: 2.581\n",
      "loss at step 22 of 50: 2.576\n",
      "loss at step 23 of 50: 2.57\n",
      "loss at step 24 of 50: 2.566\n",
      "loss at step 25 of 50: 2.561\n",
      "loss at step 26 of 50: 2.557\n",
      "loss at step 27 of 50: 2.553\n",
      "loss at step 28 of 50: 2.55\n",
      "loss at step 29 of 50: 2.546\n",
      "loss at step 30 of 50: 2.543\n",
      "loss at step 31 of 50: 2.541\n",
      "loss at step 32 of 50: 2.538\n",
      "loss at step 33 of 50: 2.536\n",
      "loss at step 34 of 50: 2.533\n",
      "loss at step 35 of 50: 2.531\n",
      "loss at step 36 of 50: 2.529\n",
      "loss at step 37 of 50: 2.527\n",
      "loss at step 38 of 50: 2.525\n",
      "loss at step 39 of 50: 2.524\n",
      "loss at step 40 of 50: 2.522\n",
      "loss at step 41 of 50: 2.521\n",
      "loss at step 42 of 50: 2.519\n",
      "loss at step 43 of 50: 2.518\n",
      "loss at step 44 of 50: 2.517\n",
      "loss at step 45 of 50: 2.515\n",
      "loss at step 46 of 50: 2.514\n",
      "loss at step 47 of 50: 2.513\n",
      "loss at step 48 of 50: 2.512\n",
      "loss at step 49 of 50: 2.511\n",
      "loss at step 50 of 50: 2.51\n",
      "loss at the end of training: 2.5101819038391113\n",
      "-----\n",
      "now printing 10 names based on our weights\n",
      "-----\n",
      "morvann.\n",
      "akela.\n",
      "az.\n",
      "arileri.\n",
      "chaiadayra.\n",
      "fbrlqwouseyton.\n",
      "zqzevbran.\n",
      "han.\n",
      "ke.\n",
      "etssleleronakfbe.\n"
     ]
    }
   ],
   "source": [
    "bigram_classifier_with_split_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f44d619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_classifier():\n",
    "    # set up the look up tables that will be used for training the neural network\n",
    "    chars = sorted(list(set(''.join(words))))\n",
    "    stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "    stoi['.'] = 0\n",
    "    itos = {i:s for s, i in stoi.items()}\n",
    "    \n",
    "    # Build the inputs and target labels\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            xs.append(ix1)\n",
    "            ys.append(ix2)\n",
    "            \n",
    "    # convert inputs and target labels as torch.tensors\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    print(f'number of examples: {xs.nelement()}')\n",
    "\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((27, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "    train_steps = 50\n",
    "    for k in range(train_steps):\n",
    "        # Forward pass\n",
    "        xenc = F.one_hot(xs, num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "\n",
    "        # Backward pass\n",
    "        W.grad = None # remember to always zero your grads\n",
    "        loss.backward()\n",
    "\n",
    "        # Update data\n",
    "        W.data += -50 * W.grad\n",
    "        print(f'loss at step {k+1} of {train_steps}: {loss.item():.4}')\n",
    "        \n",
    "    print(f'loss at the end of training: {loss}')\n",
    "    print('-----')\n",
    "    num_names = 10\n",
    "    print(f'now printing {num_names} names based on our weights')\n",
    "    print('-----')\n",
    "    for i in range(num_names):\n",
    "        out = []\n",
    "        ix = 0\n",
    "        while True:\n",
    "            xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "            logits = xenc @ W\n",
    "            counts = logits.exp()\n",
    "            probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "            ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "            out.append(itos[ix])\n",
    "            if ix == 0:\n",
    "                break\n",
    "\n",
    "        print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f143bb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "number of examples: 228146\n",
      "loss at step 1 of 50: 3.769\n",
      "loss at step 2 of 50: 3.379\n",
      "loss at step 3 of 50: 3.161\n",
      "loss at step 4 of 50: 3.027\n",
      "loss at step 5 of 50: 2.934\n",
      "loss at step 6 of 50: 2.867\n",
      "loss at step 7 of 50: 2.817\n",
      "loss at step 8 of 50: 2.777\n",
      "loss at step 9 of 50: 2.745\n",
      "loss at step 10 of 50: 2.719\n",
      "loss at step 11 of 50: 2.697\n",
      "loss at step 12 of 50: 2.677\n",
      "loss at step 13 of 50: 2.661\n",
      "loss at step 14 of 50: 2.646\n",
      "loss at step 15 of 50: 2.634\n",
      "loss at step 16 of 50: 2.622\n",
      "loss at step 17 of 50: 2.613\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbigram_classifier_with_split_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 44\u001b[0m, in \u001b[0;36mbigram_classifier_with_split_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     43\u001b[0m W\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# remember to always zero your grads\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Update data\u001b[39;00m\n\u001b[1;32m     47\u001b[0m W\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m50\u001b[39m \u001b[38;5;241m*\u001b[39m W\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m~/py/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/py/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bigram_classifier_with_split_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71b0f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'grape', 'fig', 'kiwi', 'honeydew', 'banana', 'elderberry', 'cherry', 'lemon', 'apple']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Calculate split points\\ntotal_length = len(dataset)\\nsplit_80 = int(0.8 * total_length)\\nsplit_90 = int(0.9 * total_length)\\n\\n# Split the dataset\\ntrain_data = dataset[:split_80]  # 80% for training\\nvalidation_data = dataset[split_80:split_90]  # 10% for validation\\ntest_data = dataset[split_90:]  # 10% for testing\\n\\n# Printing to verify the split\\nprint(f\"Training data ({len(train_data)}): {train_data}\")\\nprint(f\"Validation data ({len(validation_data)}): {validation_data}\")\\nprint(f\"Test data ({len(test_data)}): {test_data}\")\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Assuming you have a list of words\n",
    "dataset = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\", \"honeydew\", \"kiwi\", \"lemon\"]\n",
    "\n",
    "# Shuffle the dataset to ensure random selection\n",
    "random.shuffle(dataset)\n",
    "print(dataset)\n",
    "'''\n",
    "# Calculate split points\n",
    "total_length = len(dataset)\n",
    "split_80 = int(0.8 * total_length)\n",
    "split_90 = int(0.9 * total_length)\n",
    "\n",
    "# Split the dataset\n",
    "train_data = dataset[:split_80]  # 80% for training\n",
    "validation_data = dataset[split_80:split_90]  # 10% for validation\n",
    "test_data = dataset[split_90:]  # 10% for testing\n",
    "\n",
    "# Printing to verify the split\n",
    "print(f\"Training data ({len(train_data)}): {train_data}\")\n",
    "print(f\"Validation data ({len(validation_data)}): {validation_data}\")\n",
    "print(f\"Test data ({len(test_data)}): {test_data}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e399b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
