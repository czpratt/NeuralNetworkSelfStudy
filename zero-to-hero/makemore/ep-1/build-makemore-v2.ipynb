{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f41675",
   "metadata": {},
   "source": [
    "Part 2: Now we're going to be making a neural network that will allow us to adjust the model which predicts the next character. In this way, we don't necessarily have to rely on the counts of how often the letter appears next, but we'll be able to tune this weights according to the minimizing the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07bec2",
   "metadata": {},
   "source": [
    "First we'll create the training set for all of the bigrams. The idea is that we want the training set to be the first character of the bigram, while the second element in the training set is the predicted (target) for the next character. But we'll be forming them in terms of integers and not the strings themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c4a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63543fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b8618525",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f34c93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "ch1 ch2: . e\n",
      "stoi[ch1], stoi[ch2]: 0 5\n",
      "ch1 ch2: e m\n",
      "stoi[ch1], stoi[ch2]: 5 13\n",
      "ch1 ch2: m m\n",
      "stoi[ch1], stoi[ch2]: 13 13\n",
      "ch1 ch2: m a\n",
      "stoi[ch1], stoi[ch2]: 13 1\n",
      "ch1 ch2: a .\n",
      "stoi[ch1], stoi[ch2]: 1 0\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    print(w)\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        print(f'ch1 ch2: {ch1} {ch2}')\n",
    "        print(f'stoi[ch1], stoi[ch2]: {stoi[ch1]} {stoi[ch2]}')\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "# then we'll turn these into tensors since we'll use them to build the network with PyTorch\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "252bbfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c3342cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed084d",
   "metadata": {},
   "source": [
    "The logic is that when xs[elem] is 0, then we want the neural network to favor ys[elem] to be 5, and same for xs[elem] = 5, we want 13 to be in the target set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2b8ac",
   "metadata": {},
   "source": [
    "Next what we want to do is to be able to encode our inputs, xs, in a way that we can eventually feed them into the neural network. This is known as one_hot.\n",
    "\n",
    "One_hot is a way to be able to turn some kind of integer-labeled list, and convert it into a vector where the element of the vector will either be 1 if this matches the element where it occurs, or it will obtain a 0 if it does not occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efe5c471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example tensor of labels\n",
    "labels = torch.tensor([0, 2, 1, 3])\n",
    "\n",
    "# Number of classes (unique values in labels)\n",
    "# technically we could adjust this to not equal len(labels), but that kind of defeats the purpose of one_hot\n",
    "# since the element of the vector should correspond as to whether the element appears or not\n",
    "num_classes = 4\n",
    "\n",
    "# Perform one-hot encoding\n",
    "one_hot = F.one_hot(labels, num_classes=num_classes)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae603de4",
   "metadata": {},
   "source": [
    "Now we'll do it for our tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6eac622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ac2c321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.float32\n",
      "torch.Size([5, 27])\n"
     ]
    }
   ],
   "source": [
    "# one_hot doesn't support data type specification, so we have to manually make all of the elements\n",
    "# set to be float32 \n",
    "# float32 can be fed into neural nets so this is what we want\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "print(xenc)\n",
    "print(xenc.dtype)\n",
    "print(xenc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a1346",
   "metadata": {},
   "source": [
    "We can visualize this to make it more easy to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ccaf7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7b5bbd9568c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52715f82",
   "metadata": {},
   "source": [
    "Now we want to begin constructing our first neuron in order to compute w*x + b, where w*x will be the dot product between our weight vector, which we need to define, and our encoded inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93d434ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1874],\n",
      "        [ 0.9033],\n",
      "        [ 1.2363],\n",
      "        [ 1.5785],\n",
      "        [ 1.6088],\n",
      "        [ 1.0526],\n",
      "        [-0.9510],\n",
      "        [-1.2195],\n",
      "        [ 0.3269],\n",
      "        [-0.4024],\n",
      "        [ 0.1233],\n",
      "        [ 1.2649],\n",
      "        [ 1.2503],\n",
      "        [-0.6534],\n",
      "        [ 1.3412],\n",
      "        [ 0.1028],\n",
      "        [ 0.0777],\n",
      "        [ 0.8357],\n",
      "        [-0.7215],\n",
      "        [ 0.8080],\n",
      "        [ 1.5695],\n",
      "        [-0.5117],\n",
      "        [ 0.5022],\n",
      "        [-0.7028],\n",
      "        [ 0.4897],\n",
      "        [ 0.3373],\n",
      "        [ 1.8369]])\n",
      "torch.Size([27, 1])\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27, 1))\n",
    "print(W)\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb161e",
   "metadata": {},
   "source": [
    "The dimensions should be able to match tensor multiplication. xenc.shape is (5, 27) while W.shape is (27, 1), so the output should be a (5, 1) tensor and we'll print it afterwords\n",
    "\n",
    "Now torch.dot only works with vector multiplication, and it looks like matmul is the best way to do handle general cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b00cd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1874],\n",
       "        [ 1.0526],\n",
       "        [-0.6534],\n",
       "        [-0.6534],\n",
       "        [ 0.9033]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(xenc, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b8776d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1874],\n",
       "        [ 1.0526],\n",
       "        [-0.6534],\n",
       "        [-0.6534],\n",
       "        [ 0.9033]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a183323",
   "metadata": {},
   "source": [
    "Now instead of there being one neuron, we have 27 neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fab94997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8017, -0.9672, -0.6480, -2.2202,  0.3711,  0.5876,  1.0277, -0.3077,\n",
      "          2.3207, -0.0508,  1.9148, -0.0178, -0.5968, -0.3641,  0.3521, -0.5240,\n",
      "          0.2767, -0.5051,  0.6585,  0.9934,  1.6597,  0.4966, -1.1776,  1.5594,\n",
      "          1.6234, -1.7313, -0.6639],\n",
      "        [-1.6790, -0.5512,  2.1495,  0.5418,  0.5983, -1.8333,  0.5471,  0.8405,\n",
      "          0.4083, -1.0899,  0.1273,  1.7569,  3.0191, -2.3936,  0.7517, -0.8810,\n",
      "          0.8537,  2.1123, -0.5512,  0.4274,  0.7506,  0.6206,  0.4706, -0.5531,\n",
      "          0.2663,  0.7085,  0.5804],\n",
      "        [-0.5206,  1.2206, -1.1267,  1.0697, -1.4510,  1.2616, -0.8071, -0.7825,\n",
      "         -0.7483, -0.0658,  1.5168, -0.1249, -0.3672, -0.4147,  0.2421,  0.8449,\n",
      "         -0.8644, -0.2355,  0.1899,  0.5728, -0.4563, -1.4228,  1.8715, -0.6236,\n",
      "          0.4035, -1.6648,  1.1406],\n",
      "        [-0.5206,  1.2206, -1.1267,  1.0697, -1.4510,  1.2616, -0.8071, -0.7825,\n",
      "         -0.7483, -0.0658,  1.5168, -0.1249, -0.3672, -0.4147,  0.2421,  0.8449,\n",
      "         -0.8644, -0.2355,  0.1899,  0.5728, -0.4563, -1.4228,  1.8715, -0.6236,\n",
      "          0.4035, -1.6648,  1.1406],\n",
      "        [ 0.2502,  0.9531, -2.0348,  1.8758, -1.5621, -0.9062,  0.8331,  0.4586,\n",
      "         -0.1932, -0.6008,  1.3602, -0.6077, -1.0501,  0.7800,  0.0539, -0.0892,\n",
      "         -0.8602, -0.7749, -0.4883,  0.5371,  1.2712, -0.3306,  1.6638, -0.3936,\n",
      "          0.1723,  1.2397,  0.3365]])\n",
      "torch.Size([5, 27])\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((27, 27))\n",
    "res = xenc @ W\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b1d57",
   "metadata": {},
   "source": [
    "Now what's happened is that the five original inputs have been multiplied by our weights 27 times, instead of just once, which gives us a much more complicated result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ef22c",
   "metadata": {},
   "source": [
    "Let's look at what's happening under the hood step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f77fc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3307)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W)[3, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4cb94d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "553dc949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8283,  0.5999,  1.1059,  0.6448, -0.0031,  0.9335,  0.3229,  0.9191,\n",
       "        -1.8441,  1.0545, -1.3281, -1.9802, -0.9250,  0.3307,  1.0988, -0.6189,\n",
       "        -1.2421, -1.1420, -1.3161,  0.7108,  1.0059, -0.2576, -0.4908,  0.7756,\n",
       "         1.2626, -1.3687, -1.4941])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[:, 13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "564149c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3307)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc[3] * W[:, 13]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97565603",
   "metadata": {},
   "source": [
    "To recap, we are feeding 5 inputs into 27 neurons---this is technically the first layer of our network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb3a8b",
   "metadata": {},
   "source": [
    "Now what we want to do is to be able to interpret our neurons output in a way that makes sense in the context of neural networks. \n",
    "\n",
    "One way to do this is to look at the exponential of each element in the output of the matrix multiplcation, which allows all negative numbers to become small positive numbers, while all positive numbers become larger positive values\n",
    "\n",
    "This turns the output of the neuron into something that looks like a count, where the larger the positive value of the output from this layer of neurons, this represents how more much more likely that index will influence the next layer in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6545230b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.0599,  0.3801,  0.5231,  0.1086,  1.4493,  1.7996,  2.7946,  0.7351,\n",
       "         10.1830,  0.9505,  6.7853,  0.9824,  0.5506,  0.6948,  1.4220,  0.5922,\n",
       "          1.3187,  0.6034,  1.9319,  2.7004,  5.2578,  1.6431,  0.3080,  4.7559,\n",
       "          5.0704,  0.1771,  0.5148],\n",
       "        [ 0.1866,  0.5762,  8.5805,  1.7191,  1.8190,  0.1599,  1.7283,  2.3175,\n",
       "          1.5042,  0.3363,  1.1358,  5.7942, 20.4721,  0.0913,  2.1205,  0.4144,\n",
       "          2.3483,  8.2674,  0.5763,  1.5332,  2.1183,  1.8600,  1.6010,  0.5751,\n",
       "          1.3051,  2.0308,  1.7868],\n",
       "        [ 0.5942,  3.3893,  0.3241,  2.9144,  0.2343,  3.5311,  0.4461,  0.4573,\n",
       "          0.4732,  0.9363,  4.5576,  0.8826,  0.6927,  0.6606,  1.2740,  2.3278,\n",
       "          0.4213,  0.7901,  1.2091,  1.7732,  0.6336,  0.2410,  6.4982,  0.5360,\n",
       "          1.4971,  0.1892,  3.1288],\n",
       "        [ 0.5942,  3.3893,  0.3241,  2.9144,  0.2343,  3.5311,  0.4461,  0.4573,\n",
       "          0.4732,  0.9363,  4.5576,  0.8826,  0.6927,  0.6606,  1.2740,  2.3278,\n",
       "          0.4213,  0.7901,  1.2091,  1.7732,  0.6336,  0.2410,  6.4982,  0.5360,\n",
       "          1.4971,  0.1892,  3.1288],\n",
       "        [ 1.2842,  2.5936,  0.1307,  6.5262,  0.2097,  0.4041,  2.3004,  1.5818,\n",
       "          0.8243,  0.5484,  3.8972,  0.5446,  0.3499,  2.1814,  1.0554,  0.9147,\n",
       "          0.4231,  0.4607,  0.6137,  1.7111,  3.5650,  0.7185,  5.2795,  0.6746,\n",
       "          1.1880,  3.4544,  1.4000]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9fadf5",
   "metadata": {},
   "source": [
    "So what we're going to do is interpret the output of xenc @ W as the logits, represnting the log-counts of how often those corresponding indices are likely to show up in subsequent layers, and then the exponential of that is the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "99d39c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.0599,  0.3801,  0.5231,  0.1086,  1.4493,  1.7996,  2.7946,  0.7351,\n",
       "         10.1830,  0.9505,  6.7853,  0.9824,  0.5506,  0.6948,  1.4220,  0.5922,\n",
       "          1.3187,  0.6034,  1.9319,  2.7004,  5.2578,  1.6431,  0.3080,  4.7559,\n",
       "          5.0704,  0.1771,  0.5148],\n",
       "        [ 0.1866,  0.5762,  8.5805,  1.7191,  1.8190,  0.1599,  1.7283,  2.3175,\n",
       "          1.5042,  0.3363,  1.1358,  5.7942, 20.4721,  0.0913,  2.1205,  0.4144,\n",
       "          2.3483,  8.2674,  0.5763,  1.5332,  2.1183,  1.8600,  1.6010,  0.5751,\n",
       "          1.3051,  2.0308,  1.7868],\n",
       "        [ 0.5942,  3.3893,  0.3241,  2.9144,  0.2343,  3.5311,  0.4461,  0.4573,\n",
       "          0.4732,  0.9363,  4.5576,  0.8826,  0.6927,  0.6606,  1.2740,  2.3278,\n",
       "          0.4213,  0.7901,  1.2091,  1.7732,  0.6336,  0.2410,  6.4982,  0.5360,\n",
       "          1.4971,  0.1892,  3.1288],\n",
       "        [ 0.5942,  3.3893,  0.3241,  2.9144,  0.2343,  3.5311,  0.4461,  0.4573,\n",
       "          0.4732,  0.9363,  4.5576,  0.8826,  0.6927,  0.6606,  1.2740,  2.3278,\n",
       "          0.4213,  0.7901,  1.2091,  1.7732,  0.6336,  0.2410,  6.4982,  0.5360,\n",
       "          1.4971,  0.1892,  3.1288],\n",
       "        [ 1.2842,  2.5936,  0.1307,  6.5262,  0.2097,  0.4041,  2.3004,  1.5818,\n",
       "          0.8243,  0.5484,  3.8972,  0.5446,  0.3499,  2.1814,  1.0554,  0.9147,\n",
       "          0.4231,  0.4607,  0.6137,  1.7111,  3.5650,  0.7185,  5.2795,  0.6746,\n",
       "          1.1880,  3.4544,  1.4000]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3c84d",
   "metadata": {},
   "source": [
    "I think what's interesting is that this is all just an interpretation, because xenc wasn't obtained from using logarithms, but technically it is since log(1) = 0 while log(x) = 1 , and so we're just intepretting the matrix multiplication as forming the logits, while we take the exponential in order to reframe our perspective on what these probabilities really mean\n",
    "\n",
    "We want to take the sum of 1, since we want to take the element wise division across every element that is part of a column within every row\n",
    "\n",
    "Need to ensure that we always use keepdims=True in order to maintain the dimensionality of the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee690c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1005, 0.0063, 0.0087, 0.0018, 0.0240, 0.0298, 0.0464, 0.0122, 0.1689,\n",
       "         0.0158, 0.1125, 0.0163, 0.0091, 0.0115, 0.0236, 0.0098, 0.0219, 0.0100,\n",
       "         0.0320, 0.0448, 0.0872, 0.0273, 0.0051, 0.0789, 0.0841, 0.0029, 0.0085],\n",
       "        [0.0026, 0.0079, 0.1176, 0.0236, 0.0249, 0.0022, 0.0237, 0.0318, 0.0206,\n",
       "         0.0046, 0.0156, 0.0794, 0.2806, 0.0013, 0.0291, 0.0057, 0.0322, 0.1133,\n",
       "         0.0079, 0.0210, 0.0290, 0.0255, 0.0219, 0.0079, 0.0179, 0.0278, 0.0245],\n",
       "        [0.0146, 0.0835, 0.0080, 0.0718, 0.0058, 0.0869, 0.0110, 0.0113, 0.0117,\n",
       "         0.0231, 0.1122, 0.0217, 0.0171, 0.0163, 0.0314, 0.0573, 0.0104, 0.0195,\n",
       "         0.0298, 0.0437, 0.0156, 0.0059, 0.1600, 0.0132, 0.0369, 0.0047, 0.0770],\n",
       "        [0.0146, 0.0835, 0.0080, 0.0718, 0.0058, 0.0869, 0.0110, 0.0113, 0.0117,\n",
       "         0.0231, 0.1122, 0.0217, 0.0171, 0.0163, 0.0314, 0.0573, 0.0104, 0.0195,\n",
       "         0.0298, 0.0437, 0.0156, 0.0059, 0.1600, 0.0132, 0.0369, 0.0047, 0.0770],\n",
       "        [0.0286, 0.0578, 0.0029, 0.1456, 0.0047, 0.0090, 0.0513, 0.0353, 0.0184,\n",
       "         0.0122, 0.0869, 0.0121, 0.0078, 0.0487, 0.0235, 0.0204, 0.0094, 0.0103,\n",
       "         0.0137, 0.0382, 0.0795, 0.0160, 0.1178, 0.0150, 0.0265, 0.0770, 0.0312]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6f055ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42b45b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1005, 0.0063, 0.0087, 0.0018, 0.0240, 0.0298, 0.0464, 0.0122, 0.1689,\n",
       "        0.0158, 0.1125, 0.0163, 0.0091, 0.0115, 0.0236, 0.0098, 0.0219, 0.0100,\n",
       "        0.0320, 0.0448, 0.0872, 0.0273, 0.0051, 0.0789, 0.0841, 0.0029, 0.0085])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1b2ba",
   "metadata": {},
   "source": [
    "The way to interpret the above---at least what I'm getting---is that this row of probabilities correspond to the probability of that character corresponding to that index will appear next, after having been fed that input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a7f663b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that the columns are normalized\n",
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9cd3e23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].shape # and we see that the row just consists of 27 numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5e2a8",
   "metadata": {},
   "source": [
    "# Now we're just going to recap exactly what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30725bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddf2bf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77666ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "# then we'll turn these into tensors since we'll use them to build the network with PyTorch\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c9a20",
   "metadata": {},
   "source": [
    "Our data set will just be the first word in the names dataset for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3639c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs # these are our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79b09419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys # these are our target labels, where this is optimally what we want the neural net to optimize its weights for\n",
    "    # in order to be correctly predict the next best word that should follow the one that was given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0de9f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27816d96",
   "metadata": {},
   "source": [
    "Now we're going to make our encoding, make our logits, and obtain the probabilities that are associated with the correct letter that should follow from the input\n",
    "\n",
    "Make sure to always make your encoding floats, since one_hot doesn't support this from the method itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27b9517a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c323bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
       "        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
       "         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
       "         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
       "         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
       "         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6d954",
   "metadata": {},
   "source": [
    "The above is called the softmax! And what we did is do a forward pass in the network. From micrograd, we know how to do backgropogation, since the local probabilities were obtained using multiplication, so this will be an easy chain rule to set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3526c79",
   "metadata": {},
   "source": [
    "We can automate each part of what happens in the neural network as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4138a372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "bigram example 1: .e (indices 0 5)\n",
      "input to the neural network: 0\n",
      "output probabilities from the layer of 27 neurons: tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "the target label that we are aiming to predict next: 5\n",
      "according to the network, the probability it will show up next is: 0.0123\n",
      "this translates into a log likelihood of: -4.399273872375488\n",
      "the negative log likelihood is then 4.399273872375488\n",
      "what nlls looks like: tensor([4.3993, 0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<CopySlices>)\n",
      "-------\n",
      "bigram example 2: em (indices 5 13)\n",
      "input to the neural network: 5\n",
      "output probabilities from the layer of 27 neurons: tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "the target label that we are aiming to predict next: 13\n",
      "according to the network, the probability it will show up next is: 0.0181\n",
      "this translates into a log likelihood of: -4.014570713043213\n",
      "the negative log likelihood is then 4.014570713043213\n",
      "what nlls looks like: tensor([4.3993, 4.0146, 0.0000, 0.0000, 0.0000], grad_fn=<CopySlices>)\n",
      "-------\n",
      "bigram example 3: mm (indices 13 13)\n",
      "input to the neural network: 13\n",
      "output probabilities from the layer of 27 neurons: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "the target label that we are aiming to predict next: 13\n",
      "according to the network, the probability it will show up next is: 0.0267\n",
      "this translates into a log likelihood of: -3.623408794403076\n",
      "the negative log likelihood is then 3.623408794403076\n",
      "what nlls looks like: tensor([4.3993, 4.0146, 3.6234, 0.0000, 0.0000], grad_fn=<CopySlices>)\n",
      "-------\n",
      "bigram example 4: ma (indices 13 1)\n",
      "input to the neural network: 13\n",
      "output probabilities from the layer of 27 neurons: tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "the target label that we are aiming to predict next: 1\n",
      "according to the network, the probability it will show up next is: 0.0737\n",
      "this translates into a log likelihood of: -2.6080665588378906\n",
      "the negative log likelihood is then 2.6080665588378906\n",
      "what nlls looks like: tensor([4.3993, 4.0146, 3.6234, 2.6081, 0.0000], grad_fn=<CopySlices>)\n",
      "-------\n",
      "bigram example 5: a. (indices 1 0)\n",
      "input to the neural network: 1\n",
      "output probabilities from the layer of 27 neurons: tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "the target label that we are aiming to predict next: 0\n",
      "according to the network, the probability it will show up next is: 0.0150\n",
      "this translates into a log likelihood of: -4.201204299926758\n",
      "the negative log likelihood is then 4.201204299926758\n",
      "what nlls looks like: tensor([4.3993, 4.0146, 3.6234, 2.6081, 4.2012], grad_fn=<CopySlices>)\n",
      "----------\n",
      "average nll: 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print('-------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indices {x} {y})')\n",
    "    print(f'input to the neural network: {x}')\n",
    "    print(f'output probabilities from the layer of 27 neurons: {probs[i]}')\n",
    "    print(f'the target label that we are aiming to predict next: {y}')\n",
    "    prob = probs[i][y]\n",
    "    print(f'according to the network, the probability it will show up next is: {prob:.4f}')\n",
    "    logprob = torch.log(prob)\n",
    "    nll = -logprob\n",
    "    print(f'this translates into a log likelihood of: {logprob}')\n",
    "    print(f'the negative log likelihood is then {nll}')\n",
    "    nlls[i] = nll\n",
    "    print(f'what nlls looks like: {nlls}')\n",
    "    \n",
    "print('----------')\n",
    "print(f'average nll: {nlls.mean().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c40dc4",
   "metadata": {},
   "source": [
    "What happened was that the network has super high loss! It's not very good right now. This has to do with that W was pretty crappy with giving up weights that would matrix multiply with our encoded inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bd515",
   "metadata": {},
   "source": [
    "We used negative likelihood because we are doing classification now, when in micrograd, we did the mean squared error because that was a regression problem. Your loss function is indeed a choice, so it's kind of up to you on exactly how you want to update your weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7543e167",
   "metadata": {},
   "source": [
    "Now we're going to be doing back propogation, since we've just done the forward pass above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8633d",
   "metadata": {},
   "source": [
    " Now in order to pick out the correct probabilities that correspond to the next letter that should be correctly predicted: this should correspond to what we'd expect given that we have the target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0858996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0123, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0181, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0737, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0737, grad_fn=<SelectBackward0>),\n",
       " tensor(0.0150, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0, 5], probs[1, 13], probs[2, 1], probs[3, 1], probs[4, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c2339",
   "metadata": {},
   "source": [
    "We want an easier way to be able to obtain these probabilities in a more efficient way, and we'll do this using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39795c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.arange(5) # first we get the indices\n",
    "probs[torch.arange(5), ys] # this is incredible and funny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84540cfb",
   "metadata": {},
   "source": [
    "now that we've efficiently obtained the probabilities along with their corresponding indices, we'll now obtain the loss function for classification---the negative log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc2ceb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "loss # this matches what we got previously, but in a completely vectorized form!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e96b73",
   "metadata": {},
   "source": [
    "Now all we need is to construct the backward pass and we're good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2682dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6fc64b",
   "metadata": {},
   "source": [
    "The beauty of PyTorch is that the (digraphs?) that we used to do all of the things that we developed for micrograd, are built in-situ. This means that all of the mathematical dependencies that led to the loss function are built into a graphical structure, and when you call loss.backward(), PyTorch will perform backpropogation all the way back to the weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7617c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20d238",
   "metadata": {},
   "source": [
    "Let's check out what happened to W.grad under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12ad3500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0121,  0.0020,  0.0025,  0.0008,  0.0034, -0.1975,  0.0005,  0.0046,\n",
       "          0.0027,  0.0063,  0.0016,  0.0056,  0.0018,  0.0016,  0.0100,  0.0476,\n",
       "          0.0121,  0.0005,  0.0050,  0.0011,  0.0068,  0.0022,  0.0006,  0.0040,\n",
       "          0.0024,  0.0307,  0.0292],\n",
       "        [-0.1970,  0.0017,  0.0079,  0.0020,  0.0121,  0.0062,  0.0217,  0.0026,\n",
       "          0.0025,  0.0010,  0.0205,  0.0017,  0.0198,  0.0022,  0.0046,  0.0041,\n",
       "          0.0082,  0.0016,  0.0180,  0.0106,  0.0093,  0.0062,  0.0010,  0.0066,\n",
       "          0.0131,  0.0101,  0.0018],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0058,  0.0159,  0.0050,  0.0104,  0.0398,  0.0058,  0.0019,  0.0067,\n",
       "          0.0019,  0.0060,  0.0140,  0.0046,  0.0023, -0.1964,  0.0022,  0.0063,\n",
       "          0.0058,  0.0009,  0.0183,  0.0043,  0.0097,  0.0060,  0.0100,  0.0005,\n",
       "          0.0024,  0.0004,  0.0094],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0125, -0.1705,  0.0194,  0.0133,  0.0270,  0.0080,  0.0105,  0.0100,\n",
       "          0.0490,  0.0066,  0.0030,  0.0316,  0.0052, -0.1893,  0.0059,  0.0045,\n",
       "          0.0234,  0.0049,  0.0260,  0.0023,  0.0083,  0.0031,  0.0053,  0.0081,\n",
       "          0.0482,  0.0187,  0.0051],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad\n",
    "#torch.where(W.grad[0] == min(W.grad[0]))\n",
    "#W.grad[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e8bff",
   "metadata": {},
   "source": [
    "Only the indices that were associated with the name 'emma' were adjusted in W.grad, which makes sense, since this is the only name that the network was trained on\n",
    "\n",
    "Remember that positive gradients will increase the loss, so basically what we're essentially aiming to do with our loss function and calculating gradients is that we want the loss function to be minimized, so what needs to happen is that the gradients need to be negative---this makes sense bc it looks like most of the gradient elements that were negative ended up corresponding to the correct indices that would succeed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "847e75ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we need to update the data based on the gradient information\n",
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1dafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
