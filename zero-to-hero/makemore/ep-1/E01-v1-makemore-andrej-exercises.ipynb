{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3359c0",
   "metadata": {},
   "source": [
    "## E01: train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c3e31",
   "metadata": {},
   "source": [
    "## Now to begin our training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0e941",
   "metadata": {},
   "source": [
    "**E01:** train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffbb5f",
   "metadata": {},
   "source": [
    "### First step: How do I train a trigram model? First I need to start with trying to understand how to actually produce tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "db7e854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68bbc383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4ed29",
   "metadata": {},
   "source": [
    "Maybe, instead, I now have to split up the training set so that we are technically going to be having stoi ==> aa, ab, ac, ad, ...., wz, yz, zz, and then further make this into stoi\n",
    "\n",
    "I think that means that N will be a 27 $\\times$ 27 by 27 $\\times$ 27 tensor, since we need to take all of these combinations into account\n",
    "\n",
    "Actually it might be a 27 $\\times$ 27 by 27 matrix, because there are 27\\times27 possibilities for the input, but only 27 possibilites for the output---need to make sure the mapping is good\n",
    "\n",
    "Also we have eliminated the fact that '.' can start at the beginning, so technically we have a (27-1) $\\times 27 in the first dimension of the tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e64e4cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([728, 27])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = torch.zeros((27*27-1, 27), dtype=torch.int32)\n",
    "N.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec1301",
   "metadata": {},
   "source": [
    "Now we need to set up chars so that stoi and itos are configured properly. We should have to try to identify the next letter, so I'm not quite sure how I'll figure out how to do this yet\n",
    "\n",
    "I'm thinking we'll just need two sets of dictionaries in order to properly map everything together, since we want all of the trigrams to be referenced to unique sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "56411c2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_chr: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "single_chr = sorted(list(set(''.join(words))))\n",
    "print(f'single_chr: {single_chr}')\n",
    "single_stoi = {s:i+1 for i, s in enumerate(single_chr)} # reserve elem 0 for the dot\n",
    "single_stoi['.'] = 0\n",
    "single_itos = {i:s for s, i in single_stoi.items()} # flip integers and strings around\n",
    "\n",
    "double_chr = [f'{ch1}{ch2}' for ch1 in single_itos.values() for ch2 in single_itos.values()]\n",
    "double_stoi = {s:i+1 for i, s in enumerate(double_chr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9fd3a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to eliminate the key-value pair associated with '..' since this won't happen!\n",
    "del double_stoi['..']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "91b66fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_itos = {i:s for s, i in double_stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1e260a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch12 ch3: .e m\n",
      "ix_d, ix_s: 707, 13\n",
      "ch12 ch3: em m\n",
      "ix_d, ix_s: 121, 13\n",
      "ch12 ch3: mm a\n",
      "ix_d, ix_s: 337, 1\n",
      "ch12 ch3: ma .\n",
      "ix_d, ix_s: 325, 0\n",
      "number of examples: 4\n",
      "xs: tensor([707, 121, 337, 325])\n",
      "ys: tensor([13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "# Build your datasets\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch12 = ch1 + ch2\n",
    "        ix_d = double_stoi[ch12]\n",
    "        ix_s = single_stoi[ch3]\n",
    "        print(f'ch12 ch3: {ch12} {ch3}')\n",
    "        print(f'ix_d, ix_s: {ix_d}, {ix_s}')\n",
    "        \n",
    "        xs.append(ix_d)\n",
    "        ys.append(ix_s)\n",
    "        \n",
    "# then we'll turn these into tensors since we'll use them to build the network with PyTorch#\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f'number of examples: {xs.nelement()}')\n",
    "print(f'xs: {xs}')\n",
    "print(f'ys: {ys}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e1963",
   "metadata": {},
   "source": [
    "So we're basically going to configure this so that we want to eventually match the encoded input to the output, i.e. we want to now train our trigram classifier model!\n",
    "\n",
    "Let's do it step by step first before making an entire loop out of it---it's a good habit that I want to build from Andrej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "22816d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 27*27-1\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((num_inputs, 27), generator=g, requires_grad=True) # remember to set requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226009e",
   "metadata": {},
   "source": [
    "We need to take this a little slower\n",
    "N.shape is 728 x 27 \\\n",
    "xenc.shape is 4 x 728 \\\n",
    "W.shape is 728 x 27 \\\n",
    "so our logits are logits = xenc @ W is 4 x 27 so this will work. How do I intrepret this, though? These are now the logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be414fd",
   "metadata": {},
   "source": [
    "The ordering of our forward pass is as follows: \n",
    "\n",
    "Encode our inputs \\\n",
    "Matrix multiply with weights in order to get our logits, i.e. W $\\times x \\\n",
    "Exponentiate our logits to obtain our counts \\\n",
    "Normalize our counts, and finish our softmax procedure \\\n",
    "Then obtain the loss by taking the log likelihood of our probabilities \\\n",
    "Finally obtain the average negative log likelihood by taking the mean, and then negating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b8378843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0157, 0.0302, 0.0059, 0.0085, 0.1071, 0.0358, 0.0426, 0.0234, 0.0086,\n",
      "         0.0893, 0.0781, 0.0770, 0.1513, 0.0208, 0.0297, 0.0058, 0.0717, 0.0063,\n",
      "         0.0377, 0.0274, 0.0232, 0.0126, 0.0261, 0.0064, 0.0262, 0.0303, 0.0025],\n",
      "        [0.0139, 0.0094, 0.0397, 0.0686, 0.0444, 0.0050, 0.0111, 0.0245, 0.0102,\n",
      "         0.0206, 0.0026, 0.0147, 0.1970, 0.0124, 0.0788, 0.0459, 0.0263, 0.0025,\n",
      "         0.0579, 0.0161, 0.0223, 0.0106, 0.0627, 0.0577, 0.0516, 0.0420, 0.0516],\n",
      "        [0.0226, 0.0113, 0.1661, 0.0172, 0.0140, 0.0320, 0.0641, 0.0233, 0.0447,\n",
      "         0.0064, 0.0360, 0.0593, 0.0596, 0.0058, 0.0209, 0.0053, 0.0066, 0.0700,\n",
      "         0.0120, 0.0155, 0.0279, 0.1599, 0.0406, 0.0021, 0.0576, 0.0071, 0.0124],\n",
      "        [0.0119, 0.0322, 0.0122, 0.0629, 0.0382, 0.0812, 0.0404, 0.0082, 0.0089,\n",
      "         0.0134, 0.0104, 0.0380, 0.0203, 0.0810, 0.0116, 0.0262, 0.0185, 0.0412,\n",
      "         0.0473, 0.0329, 0.1842, 0.0691, 0.0493, 0.0120, 0.0261, 0.0094, 0.0127]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=num_inputs).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "print(probs)\n",
    "#loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "#loss # looks super high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c7d7d",
   "metadata": {},
   "source": [
    "Now let's do our backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "277581e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a3fd6f",
   "metadata": {},
   "source": [
    "Finally, we need to update our data according to the gradient of W which we just computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "98fa9cf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m W\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "W.data += -1 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda87952",
   "metadata": {},
   "source": [
    "Run the below in order to compare the original loss with the new loss after doing the training 'loop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7b96ef6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2948, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=num_inputs).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "loss # looks super high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb338f0",
   "metadata": {},
   "source": [
    "### Now let's formulate this as a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "55c928ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg neg log likelihood loss in step 0 of 10: 4.2948\n",
      "avg neg log likelihood loss in step 1 of 10: 4.1640\n",
      "avg neg log likelihood loss in step 2 of 10: 4.0339\n",
      "avg neg log likelihood loss in step 3 of 10: 3.9045\n",
      "avg neg log likelihood loss in step 4 of 10: 3.7758\n",
      "avg neg log likelihood loss in step 5 of 10: 3.6480\n",
      "avg neg log likelihood loss in step 6 of 10: 3.5211\n",
      "avg neg log likelihood loss in step 7 of 10: 3.3953\n",
      "avg neg log likelihood loss in step 8 of 10: 3.2706\n",
      "avg neg log likelihood loss in step 9 of 10: 3.1472\n"
     ]
    }
   ],
   "source": [
    "train_steps = 10\n",
    "train_step = 0.5\n",
    "num_inputs = 27*27-1\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((num_inputs, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "for k in range(train_steps):\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=num_inputs).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    anll = -probs[torch.arange(len(ys)), ys].log().mean() # average neg log likelihood\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None\n",
    "    anll.backward()\n",
    "    \n",
    "    # Update\n",
    "    W.data += -train_step * W.grad\n",
    "    print(f'avg neg log likelihood loss in step {k} of {train_steps}: {anll:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7799598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6c3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
