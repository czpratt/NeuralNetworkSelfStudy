{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f41675",
   "metadata": {},
   "source": [
    "Part 4, because now we're going to turn this into a full scale training loop, and i think getting to this step is super bad ass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "688562ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e401d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4b1d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "print(max(itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9320b8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# Build your datasets\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "# then we'll turn these into tensors since we'll use them to build the network with PyTorch\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(f'number of examples: {xs.nelement()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "637ddb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26)\n",
      "loss at step 1 of 10: 3.759\n",
      "loss at step 2 of 10: 3.371\n",
      "loss at step 3 of 10: 3.154\n",
      "loss at step 4 of 10: 3.02\n",
      "loss at step 5 of 10: 2.928\n",
      "loss at step 6 of 10: 2.86\n",
      "loss at step 7 of 10: 2.81\n",
      "loss at step 8 of 10: 2.77\n",
      "loss at step 9 of 10: 2.738\n",
      "loss at step 10 of 10: 2.711\n"
     ]
    }
   ],
   "source": [
    "# Let's build our training loop\n",
    "\n",
    "# initialize your random number generator from which you will obtain weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "print(max(xs))\n",
    "\n",
    "train_steps = 10\n",
    "for k in range(train_steps):\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None # remember to always zero your grads\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update data\n",
    "    W.data += -50 * W.grad\n",
    "    print(f'loss at step {k+1} of {train_steps}: {loss.item():.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f83ff",
   "metadata": {},
   "source": [
    "We are supposed to be able to approach the original loss value that we got when we were just counting how many times a character would appear in the dataset afte smoothing, this was about 2.47, which is exactly what I'm getting now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048cfce6",
   "metadata": {},
   "source": [
    "There's this awesome concept called regularization, which we can add onto our gradient descent algorithm to train our network. It's equivalent to making the smoothing in the `counting' scenario go from N+1 -> N+1e5, since all  this does is make all of the probabilities go to the uniform distribution\n",
    "\n",
    "What we are after now is to be able to try to push all of the weights of W to go to zero. What this does is it sends the logits ==> exp.() to 1, so that the probabilities end up becoming the uniform distribution, and thus completely matching the uniform case for the counting scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4146a209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 1 of 10: 3.769\n",
      "loss at step 2 of 10: 3.379\n",
      "loss at step 3 of 10: 3.161\n",
      "loss at step 4 of 10: 3.027\n",
      "loss at step 5 of 10: 2.934\n",
      "loss at step 6 of 10: 2.867\n",
      "loss at step 7 of 10: 2.817\n",
      "loss at step 8 of 10: 2.777\n",
      "loss at step 9 of 10: 2.745\n",
      "loss at step 10 of 10: 2.719\n"
     ]
    }
   ],
   "source": [
    "# Let's build our training loop\n",
    "\n",
    "# initialize your random number generator from which you will obtain weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True) # remember to set requires_grad\n",
    "\n",
    "train_steps = 10\n",
    "for k in range(train_steps):\n",
    "    # Forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(ys)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None # remember to always zero your grads\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update data\n",
    "    W.data += -50 * W.grad\n",
    "    print(f'loss at step {k+1} of {train_steps}: {loss.item():.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920801e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ce9f8d5",
   "metadata": {},
   "source": [
    "And now, let's sample from the neural network and see what kind of results we get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37b7a5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bafb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
