{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb3b113e",
   "metadata": {},
   "source": [
    "### Version 2, since this is a good checkpoint and it's good to summarize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84e0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e4a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d637296",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3 # context length: adjust how many characters will be used to predict the next one\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    #print(f'word: {w}')\n",
    "    context = [0] * block_size\n",
    "    \n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a895a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn(27, 2)\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5960dff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.5610)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, emb.shape[1]*emb.shape[2]) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(dim=1, keepdims=True)\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4711dd3",
   "metadata": {},
   "source": [
    "Now introducing the cross entorpy as a way to make this process faster, and to save all of this code! And we get the same answer as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e8b99a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.3348)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60192bb7",
   "metadata": {},
   "source": [
    "Using cross_entropy is great because (1) the \"educational way\" of calculating loss because there's many intermediate tensors that need to be created in the process of calculating loss, which is inefficient. cross_entropy has dedicated kernals which can very efficiently evaluate these tensor operations without needing intermediate tensors (2) this incredibly clever idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48a5e846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4956e-08, 2.2197e-06, 1.7986e-02, 9.8201e-01])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose we have the logits\n",
    "logits = torch.tensor([-10, -5, 4, 8])\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum()\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d1af6",
   "metadata": {},
   "source": [
    "This is all well and good, and everything is well behaved when we take the exponential of the logits because large negative numbers go to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b67b1806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0180, 0.9820])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose we have the logits\n",
    "logits = torch.tensor([-100, -100, 4, 8])\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum()\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b531a78",
   "metadata": {},
   "source": [
    "But we will get very bad answers if our logits are large positive numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b73bb5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose we have the logits\n",
    "logits = torch.tensor([-100, -100, 4, 100])\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum()\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a2ca15",
   "metadata": {},
   "source": [
    "and this is because:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ff92232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7835e-44, 3.7835e-44, 5.4598e+01,        inf])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts # we have an infinity here because there is not enough digits of precision to represent that element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58292e",
   "metadata": {},
   "source": [
    "So what PyTorch does under the hood is get the maximum of the passed tensor, and then subtracts the entire tensor from that, since probabilities will be the same even if the largest is added to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35a28028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 2.0305e-42, 1.0000e+00])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose we have the logits\n",
    "logits = torch.tensor([-100, -100, 4, 100]) - 100\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum()\n",
    "prob # bam, well behaved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02278f",
   "metadata": {},
   "source": [
    "### So with that said let's redefine how we can calculate our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc58fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn(27, 2)\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)\n",
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4693bd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.3392, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "h = torch.tanh(emb.view(-1, emb.shape[1]*emb.shape[2]) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f75395",
   "metadata": {},
   "source": [
    "### Always remember to zero your gradients! Always remember to requires_grad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a362d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.3392, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.backward()\n",
    "# update\n",
    "for p in parameters:\n",
    "    p.data += -0.1 * p.grad\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fb661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
