{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84e0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a284ac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "432c5034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(set(''.join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e4a67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6e35c",
   "metadata": {},
   "source": [
    "Now let's build the dataset\n",
    "\n",
    "How the code works is the following:\n",
    "\n",
    "- block_size specifies the context length, i.e. the length of characters that will be used to predict the next one\n",
    "- context initially starts off as a list of dots. as you increment the context, you add the subsequent characters into the context, specifically the indices\n",
    "- personally i think the print statement is genius. it really visualizes what's going on in context, and it helps me understand how itos really works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d637296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: emma\n",
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "word: olivia\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "word: ava\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n",
      "word: isabella\n",
      "... ---> i\n",
      "..i ---> s\n",
      ".is ---> a\n",
      "isa ---> b\n",
      "sab ---> e\n",
      "abe ---> l\n",
      "bel ---> l\n",
      "ell ---> a\n",
      "lla ---> .\n",
      "word: sophia\n",
      "... ---> s\n",
      "..s ---> o\n",
      ".so ---> p\n",
      "sop ---> h\n",
      "oph ---> i\n",
      "phi ---> a\n",
      "hia ---> .\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # context length: adjust how many characters will be used to predict the next one\n",
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(f'word: {w}')\n",
    "    context = [0] * block_size\n",
    "    \n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bac75ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's look at the specifics of our dataset\n",
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa19e44",
   "metadata": {},
   "source": [
    "In the paper that we are reproducing, they embed 17,000 words in as little as 30 dimensions, so since we only have 27 characters to worry about, we'll start by embedding them in 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "220b8ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5065, -1.1483],\n",
       "         [ 0.8822,  0.5330],\n",
       "         [-1.2642,  0.0869],\n",
       "         [-0.0562, -0.3326],\n",
       "         [ 0.0144,  1.2099],\n",
       "         [ 1.8999, -1.2768],\n",
       "         [ 0.1334,  0.1299],\n",
       "         [ 0.4965, -0.9700],\n",
       "         [ 0.7935,  0.0548],\n",
       "         [-0.6443,  1.4054],\n",
       "         [-1.5379, -0.0958],\n",
       "         [-1.1648, -0.5040],\n",
       "         [-0.8052,  1.8072],\n",
       "         [-1.3181,  0.5950],\n",
       "         [-0.3643,  1.0934],\n",
       "         [ 0.1604,  0.3971],\n",
       "         [ 0.9695, -0.4810],\n",
       "         [ 0.4978,  2.4461],\n",
       "         [-0.7249, -2.0963],\n",
       "         [ 0.4252, -0.4802],\n",
       "         [ 0.0848, -0.2283],\n",
       "         [-1.0108,  1.3149],\n",
       "         [-0.5131,  0.0509],\n",
       "         [ 1.4330,  1.0075],\n",
       "         [ 0.6535, -0.2736],\n",
       "         [-0.6682,  0.1172],\n",
       "         [-0.2573, -0.3574]]),\n",
       " torch.Size([27, 2]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.randn((27, 2))\n",
    "C, C.shape, C.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d4910b",
   "metadata": {},
   "source": [
    "Now say if we wanted to embed values into our C matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c03cb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.8999, -1.2768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5] # this picks out the 5th row vector in C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25a3ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8999, -1.2768],\n",
      "        [ 0.1334,  0.1299],\n",
      "        [ 0.4965, -0.9700]]) \n",
      " tensor([[ 1.8999, -1.2768],\n",
      "        [ 0.1334,  0.1299],\n",
      "        [ 0.4965, -0.9700]]) \n",
      " tensor([[ 1.8999, -1.2768],\n",
      "        [ 0.1334,  0.1299],\n",
      "        [ 0.4965, -0.9700],\n",
      "        [ 0.4965, -0.9700],\n",
      "        [-0.6443,  1.4054]])\n"
     ]
    }
   ],
   "source": [
    "# and let's further say that we wanted to retrieve multiple rows of C to basically pick out \n",
    "# some of these row vectors\n",
    "# this is because pytorch indexing is very powerful\n",
    "# i can see how this can be used for being able to pick out multiple rows of C at a time when given X\n",
    "print(C[[5,6,7]], '\\n', C[torch.tensor([5,6,7])], '\\n', C[torch.tensor([5,6,7,7,9])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67cbafdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [ 1.8999, -1.2768]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [ 1.8999, -1.2768],\n",
       "         [-1.3181,  0.5950]],\n",
       "\n",
       "        [[ 1.8999, -1.2768],\n",
       "         [-1.3181,  0.5950],\n",
       "         [-1.3181,  0.5950]],\n",
       "\n",
       "        [[-1.3181,  0.5950],\n",
       "         [-1.3181,  0.5950],\n",
       "         [ 0.8822,  0.5330]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [ 0.1604,  0.3971]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [ 0.1604,  0.3971],\n",
       "         [-0.8052,  1.8072]],\n",
       "\n",
       "        [[ 0.1604,  0.3971],\n",
       "         [-0.8052,  1.8072],\n",
       "         [-0.6443,  1.4054]],\n",
       "\n",
       "        [[-0.8052,  1.8072],\n",
       "         [-0.6443,  1.4054],\n",
       "         [-0.5131,  0.0509]],\n",
       "\n",
       "        [[-0.6443,  1.4054],\n",
       "         [-0.5131,  0.0509],\n",
       "         [-0.6443,  1.4054]],\n",
       "\n",
       "        [[-0.5131,  0.0509],\n",
       "         [-0.6443,  1.4054],\n",
       "         [ 0.8822,  0.5330]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [ 0.8822,  0.5330]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [ 0.8822,  0.5330],\n",
       "         [-0.5131,  0.0509]],\n",
       "\n",
       "        [[ 0.8822,  0.5330],\n",
       "         [-0.5131,  0.0509],\n",
       "         [ 0.8822,  0.5330]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [-0.6443,  1.4054]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.6443,  1.4054],\n",
       "         [ 0.4252, -0.4802]],\n",
       "\n",
       "        [[-0.6443,  1.4054],\n",
       "         [ 0.4252, -0.4802],\n",
       "         [ 0.8822,  0.5330]],\n",
       "\n",
       "        [[ 0.4252, -0.4802],\n",
       "         [ 0.8822,  0.5330],\n",
       "         [-1.2642,  0.0869]],\n",
       "\n",
       "        [[ 0.8822,  0.5330],\n",
       "         [-1.2642,  0.0869],\n",
       "         [ 1.8999, -1.2768]],\n",
       "\n",
       "        [[-1.2642,  0.0869],\n",
       "         [ 1.8999, -1.2768],\n",
       "         [-0.8052,  1.8072]],\n",
       "\n",
       "        [[ 1.8999, -1.2768],\n",
       "         [-0.8052,  1.8072],\n",
       "         [-0.8052,  1.8072]],\n",
       "\n",
       "        [[-0.8052,  1.8072],\n",
       "         [-0.8052,  1.8072],\n",
       "         [ 0.8822,  0.5330]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [-0.5065, -1.1483],\n",
       "         [ 0.4252, -0.4802]],\n",
       "\n",
       "        [[-0.5065, -1.1483],\n",
       "         [ 0.4252, -0.4802],\n",
       "         [ 0.1604,  0.3971]],\n",
       "\n",
       "        [[ 0.4252, -0.4802],\n",
       "         [ 0.1604,  0.3971],\n",
       "         [ 0.9695, -0.4810]],\n",
       "\n",
       "        [[ 0.1604,  0.3971],\n",
       "         [ 0.9695, -0.4810],\n",
       "         [ 0.7935,  0.0548]],\n",
       "\n",
       "        [[ 0.9695, -0.4810],\n",
       "         [ 0.7935,  0.0548],\n",
       "         [-0.6443,  1.4054]],\n",
       "\n",
       "        [[ 0.7935,  0.0548],\n",
       "         [-0.6443,  1.4054],\n",
       "         [ 0.8822,  0.5330]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X] # this picks out three row vectors per vector of X, and forms its own tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bbec479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][0] # this corresponds to all of the dots in the beginning of the first word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beeb2be",
   "metadata": {},
   "source": [
    "Let's do more examples with pytorch indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb9f326e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13] # this is the thirteenth element of X\n",
    "X[13, 2] # this gets the second element of the thirteenth element of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47c4dd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8822, 0.5330])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X[13,2]] # this is the embedding of this value within X for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6363c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8822, 0.5330])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b316db6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8822, 0.5330])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1] # this matches the X element's embedding within C!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a450dcc",
   "metadata": {},
   "source": [
    "now let's formally define our embedding\n",
    "\n",
    "This is saying that we have 32 characters that are being fed into our input neurons (3), and each neuron contains a two dimensional embedding of the character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd0c2093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35557df8",
   "metadata": {},
   "source": [
    "Now let's start making our weights\n",
    "\n",
    "first dimension of W needs to equal the number of inputs that's coming from C: since we have two dimensional embeddings, and our context length was 3, the first dimension must be $3\\times2 = 6$ to handle all of the inputs\n",
    "\n",
    "the second dimension of W1 will be some arbitrary number representing the neurons in this hidden layer\n",
    "\n",
    "the biases have to equal the number of neurons that I set in W1---just a vector of scalar elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57851cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a0107",
   "metadata": {},
   "source": [
    "Our normal way of tensor multiplication won't work now, however, since emb @ W1 doesn't have compatible shapes\n",
    "\n",
    "Remember that the shape of W1 is: first dimension is the amount of characters that we saved, the second dimension is the context length---i.e. the number of input neurons, and the third dimension was the desired embedding of each character, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c124d585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 2]), torch.Size([6, 100]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape, W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f89ef70",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "emb @ W1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47963ac6",
   "metadata": {},
   "source": [
    "So now we need to do something else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d56fae",
   "metadata": {},
   "source": [
    "Now what would work is if we concatenated emb so that we could turn the 32x3x2 into a _x6 tensor so that we can do the tensor multiplication\n",
    "\n",
    "The first step we need to do is pluck out the first embedding, but keep all of the relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "715521dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((emb[:, 0], emb[:, 1], emb[:, 2]), dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be12b8",
   "metadata": {},
   "source": [
    "However this isn't good enough, since we'd have to change the .cat if the context length changed, so we need something else---we basically are isolating dim=1, so we can use torch.unbind for this\n",
    "\n",
    "this creates a tuple of tensors, which is equivalent to the arg within torch.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea968253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483,  1.8999, -1.2768],\n",
       "        [-0.5065, -1.1483,  1.8999, -1.2768, -1.3181,  0.5950],\n",
       "        [ 1.8999, -1.2768, -1.3181,  0.5950, -1.3181,  0.5950],\n",
       "        [-1.3181,  0.5950, -1.3181,  0.5950,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483,  0.1604,  0.3971],\n",
       "        [-0.5065, -1.1483,  0.1604,  0.3971, -0.8052,  1.8072],\n",
       "        [ 0.1604,  0.3971, -0.8052,  1.8072, -0.6443,  1.4054],\n",
       "        [-0.8052,  1.8072, -0.6443,  1.4054, -0.5131,  0.0509],\n",
       "        [-0.6443,  1.4054, -0.5131,  0.0509, -0.6443,  1.4054],\n",
       "        [-0.5131,  0.0509, -0.6443,  1.4054,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483,  0.8822,  0.5330, -0.5131,  0.0509],\n",
       "        [ 0.8822,  0.5330, -0.5131,  0.0509,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.6443,  1.4054],\n",
       "        [-0.5065, -1.1483, -0.6443,  1.4054,  0.4252, -0.4802],\n",
       "        [-0.6443,  1.4054,  0.4252, -0.4802,  0.8822,  0.5330],\n",
       "        [ 0.4252, -0.4802,  0.8822,  0.5330, -1.2642,  0.0869],\n",
       "        [ 0.8822,  0.5330, -1.2642,  0.0869,  1.8999, -1.2768],\n",
       "        [-1.2642,  0.0869,  1.8999, -1.2768, -0.8052,  1.8072],\n",
       "        [ 1.8999, -1.2768, -0.8052,  1.8072, -0.8052,  1.8072],\n",
       "        [-0.8052,  1.8072, -0.8052,  1.8072,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483,  0.4252, -0.4802],\n",
       "        [-0.5065, -1.1483,  0.4252, -0.4802,  0.1604,  0.3971],\n",
       "        [ 0.4252, -0.4802,  0.1604,  0.3971,  0.9695, -0.4810],\n",
       "        [ 0.1604,  0.3971,  0.9695, -0.4810,  0.7935,  0.0548],\n",
       "        [ 0.9695, -0.4810,  0.7935,  0.0548, -0.6443,  1.4054],\n",
       "        [ 0.7935,  0.0548, -0.6443,  1.4054,  0.8822,  0.5330]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb, dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567fccb",
   "metadata": {},
   "source": [
    "We're starting to go into an aside, so that we can learn what's going on with torch.tensor under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74fcbc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca20444b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30469bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a28e86d",
   "metadata": {},
   "source": [
    "Now we can call .storage() and see how these tensors are stored in the computers memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e663d21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6962/806127742.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  a.storage() # computers store vectors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage() # computers store vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe8923",
   "metadata": {},
   "source": [
    "So turns out we can just use .view() instead of using the .cat operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48ce71aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483,  1.8999, -1.2768],\n",
       "        [-0.5065, -1.1483,  1.8999, -1.2768, -1.3181,  0.5950],\n",
       "        [ 1.8999, -1.2768, -1.3181,  0.5950, -1.3181,  0.5950],\n",
       "        [-1.3181,  0.5950, -1.3181,  0.5950,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483,  0.1604,  0.3971],\n",
       "        [-0.5065, -1.1483,  0.1604,  0.3971, -0.8052,  1.8072],\n",
       "        [ 0.1604,  0.3971, -0.8052,  1.8072, -0.6443,  1.4054],\n",
       "        [-0.8052,  1.8072, -0.6443,  1.4054, -0.5131,  0.0509],\n",
       "        [-0.6443,  1.4054, -0.5131,  0.0509, -0.6443,  1.4054],\n",
       "        [-0.5131,  0.0509, -0.6443,  1.4054,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483,  0.8822,  0.5330, -0.5131,  0.0509],\n",
       "        [ 0.8822,  0.5330, -0.5131,  0.0509,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.6443,  1.4054],\n",
       "        [-0.5065, -1.1483, -0.6443,  1.4054,  0.4252, -0.4802],\n",
       "        [-0.6443,  1.4054,  0.4252, -0.4802,  0.8822,  0.5330],\n",
       "        [ 0.4252, -0.4802,  0.8822,  0.5330, -1.2642,  0.0869],\n",
       "        [ 0.8822,  0.5330, -1.2642,  0.0869,  1.8999, -1.2768],\n",
       "        [-1.2642,  0.0869,  1.8999, -1.2768, -0.8052,  1.8072],\n",
       "        [ 1.8999, -1.2768, -0.8052,  1.8072, -0.8052,  1.8072],\n",
       "        [-0.8052,  1.8072, -0.8052,  1.8072,  0.8822,  0.5330],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483, -0.5065, -1.1483],\n",
       "        [-0.5065, -1.1483, -0.5065, -1.1483,  0.4252, -0.4802],\n",
       "        [-0.5065, -1.1483,  0.4252, -0.4802,  0.1604,  0.3971],\n",
       "        [ 0.4252, -0.4802,  0.1604,  0.3971,  0.9695, -0.4810],\n",
       "        [ 0.1604,  0.3971,  0.9695, -0.4810,  0.7935,  0.0548],\n",
       "        [ 0.9695, -0.4810,  0.7935,  0.0548, -0.6443,  1.4054],\n",
       "        [ 0.7935,  0.0548, -0.6443,  1.4054,  0.8822,  0.5330]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a37a55",
   "metadata": {},
   "source": [
    "So now this is all what we need to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da76c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h = emb.view(32, 6) @ W1 + b1 # least efficient way of doing this\n",
    "#h = emb.view(emb.shape[0], emb.shape[1]*emb.shape[2]) @ W1 + b1 # getting better\n",
    "h = emb.view(-1, emb.shape[1]*emb.shape[2]) @ W1 + b1 # pytorch will infer the size that h needs to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb0ae2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7eebfc",
   "metadata": {},
   "source": [
    "This is a much better way to manipulate tensors because torch.cat needs more memory to create this new tensor, and then further do manipulations with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95832861",
   "metadata": {},
   "source": [
    "but remember that we want h to be tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7e1dbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9961,  0.6846,  0.9854,  ..., -0.2181, -0.7957, -0.9717],\n",
       "        [-0.4027, -0.1303,  0.9906,  ...,  0.9999, -0.9998, -0.9985],\n",
       "        [ 0.8235,  0.8517,  0.9596,  ..., -0.9883, -0.9744,  0.4970],\n",
       "        ...,\n",
       "        [ 0.9917,  0.2004,  0.9947,  ...,  0.9733, -0.4136, -0.7867],\n",
       "        [ 0.8529,  0.8911,  0.5631,  ..., -0.9920, -0.4192, -0.8091],\n",
       "        [-0.6496, -0.2859, -0.6388,  ...,  0.8515,  0.7850, -0.9923]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(-1, emb.shape[1]*emb.shape[2]) @ W1 + b1)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d77d63d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape # hidden layer of activations for every one of our 32 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e21fd2",
   "metadata": {},
   "source": [
    "Remember to be careful with tensor broadcasting (when I'm using addition)\n",
    "you must make sure that the shapes are able to be broadcasted together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e04f5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h.shape = [32, 100]\n",
    "# b1.shape = [100]\n",
    "# so how these will be broadcasted is the following\n",
    "# 32, 100\n",
    "#     100 (align at the right)\n",
    "\n",
    "# 32, 100\n",
    "#  1, 100 # create a fake dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26239e7b",
   "metadata": {},
   "source": [
    "Now we're going to create the last layer in the network where we're going to SoftMax the hell out our activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11d861d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2328, -1.9877, -0.2004, -0.2033,  0.1437, -1.9307, -0.0372, -0.8194,\n",
       "        -2.8052,  0.3659, -0.5587,  0.0999, -1.5277,  0.8663,  0.8419, -0.3162,\n",
       "         0.5580, -1.1929,  0.4629, -0.3704,  0.2857,  1.5748, -0.8777, -0.4344,\n",
       "        -1.6901, -0.6643, -1.7464])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2 = torch.randn((100, 27)) # 100 for number of neurons, 27 is possible number of characters\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc97bc3",
   "metadata": {},
   "source": [
    "And now to make are logits, which are the output from the activation layer (our hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1c1d3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb012c5",
   "metadata": {},
   "source": [
    "Just like before we need to exponentiate our logits, and then normalize them in order to get a probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8d9a4d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287fe32b",
   "metadata": {},
   "source": [
    "counts by itself is a $32 \\times 27$ tensor, and when we normalize, we are trying to sum up all of the elements that correspond to the particular row which itself represents a character. these are basically vectors, and they live within counts, and this is how you properly normalize. it's backwards from row comma column, those are matries, these are tensors. Each of the 32 tensors has a length 27 vector in it\n",
    "\n",
    "so you need to be careful how you sum. you have to sum on dim=1, since we want to sum all of the row elements within each tensor, keeping dim=0 will sum up particular elements in a column, which isn't correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "73eb7fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.sum(dim=1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "594ce992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1214.3671)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is not what you want to do\n",
    "counts.sum(dim=0, keepdim=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2fee6bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1214.3671)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because it leads to this\n",
    "counts[:, 0].sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0f1651e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(548391.6875)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is correct\n",
    "counts.sum(dim=1, keepdim=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6876526a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which leads to us learning how to properly normalize\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(dim=1, keepdims=True)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27274024",
   "metadata": {},
   "source": [
    "We can check to make sure that every row should sum to one now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3980a2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.sum(dim=1) # and it does!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b763b97",
   "metadata": {},
   "source": [
    "Now we need to understand how to take Y into account (our desired predictd labels)\n",
    "\n",
    "We have to iterate through prob, and then pluck out the associated probability that corresponds to the particular label Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cc7b1bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4699e-06, 2.0522e-02, 1.2604e-07, 2.1381e-13, 6.5617e-07, 1.6194e-07,\n",
       "        5.0987e-13, 3.4829e-04, 6.4209e-05, 1.0275e-11, 2.4695e-08, 2.0583e-09,\n",
       "        1.6098e-04, 5.8448e-11, 4.7563e-08, 3.7173e-13, 8.8967e-01, 9.6893e-06,\n",
       "        2.8236e-08, 6.4799e-14, 1.1435e-06, 3.1050e-16, 1.0801e-08, 1.5387e-13,\n",
       "        4.0439e-13, 1.2600e-07, 5.0489e-06, 4.6009e-10, 9.6622e-12, 1.8915e-07,\n",
       "        5.2537e-13, 3.6565e-14])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf39b95",
   "metadata": {},
   "source": [
    "Now we turn it into a loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "99b6f379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.1586)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df3a97",
   "metadata": {},
   "source": [
    "This is the loss which we want to minimize in order to get the network to predict the next sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44cdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
