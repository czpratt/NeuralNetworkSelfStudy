{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7900634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971be252",
   "metadata": {},
   "source": [
    "Step 1: Import the text, convert into data set, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3af063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the data set from the input\n",
    "# all chars are impt, even spaces, since we want the model to learn that\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "char_data = sorted(list(set(text)))\n",
    "vocab_size = len(char_data)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a391b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we make the look up table which maps either str->int, or vice versa\n",
    "stoi = { s:i for i, s in enumerate(char_data)}\n",
    "itos = { i:s for i, s in enumerate(char_data)}\n",
    "\n",
    "# encode and decode functions based on a passed list of chars\n",
    "# remember for the decoder, you want this to output a string\n",
    "# so you have to convert the list of chars into a str\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda s: ''.join([itos[i] for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f541b",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize the data set\n",
    "\n",
    "### 2.1: Encode the dataset as a torch.tensor, then split into train and val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b14cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586bf7b4",
   "metadata": {},
   "source": [
    "### 2.2 Initiate a block size for computational efficiency. As a small example, make the outputs predict the next token based on our set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac0afe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context to model: tensor([18]), target for model to learn: 47\n",
      "context to model: tensor([18, 47]), target for model to learn: 56\n",
      "context to model: tensor([18, 47, 56]), target for model to learn: 57\n",
      "context to model: tensor([18, 47, 56, 57]), target for model to learn: 58\n",
      "context to model: tensor([18, 47, 56, 57, 58]), target for model to learn: 1\n",
      "context to model: tensor([18, 47, 56, 57, 58,  1]), target for model to learn: 15\n",
      "context to model: tensor([18, 47, 56, 57, 58,  1, 15]), target for model to learn: 47\n",
      "context to model: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target for model to learn: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "# remember to seperate input x, and ideal output y, from what the model will use\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1] # shift by one\n",
    "\n",
    "# now create context for the model, and target for the model,\n",
    "# based on ideal inputs and outputs\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f'context to model: {context}, target for model to learn: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0734eefe",
   "metadata": {},
   "source": [
    "### 2.3: Now we want to learn the data in terms of batches. This is recreating how the model will be learning the inputs. We formulate the inputs as context to the model, and then we have targets that the model will learn which we write as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3708ee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "------\n",
      "context: tensor([24]), target: 43\n",
      "context: tensor([24, 43]), target: 58\n",
      "context: tensor([24, 43, 58]), target: 5\n",
      "context: tensor([24, 43, 58,  5]), target: 57\n",
      "context: tensor([24, 43, 58,  5, 57]), target: 1\n",
      "context: tensor([24, 43, 58,  5, 57,  1]), target: 46\n",
      "context: tensor([24, 43, 58,  5, 57,  1, 46]), target: 43\n",
      "context: tensor([24, 43, 58,  5, 57,  1, 46, 43]), target: 39\n",
      "context: tensor([44]), target: 53\n",
      "context: tensor([44, 53]), target: 56\n",
      "context: tensor([44, 53, 56]), target: 1\n",
      "context: tensor([44, 53, 56,  1]), target: 58\n",
      "context: tensor([44, 53, 56,  1, 58]), target: 46\n",
      "context: tensor([44, 53, 56,  1, 58, 46]), target: 39\n",
      "context: tensor([44, 53, 56,  1, 58, 46, 39]), target: 58\n",
      "context: tensor([44, 53, 56,  1, 58, 46, 39, 58]), target: 1\n",
      "context: tensor([52]), target: 58\n",
      "context: tensor([52, 58]), target: 1\n",
      "context: tensor([52, 58,  1]), target: 58\n",
      "context: tensor([52, 58,  1, 58]), target: 46\n",
      "context: tensor([52, 58,  1, 58, 46]), target: 39\n",
      "context: tensor([52, 58,  1, 58, 46, 39]), target: 58\n",
      "context: tensor([52, 58,  1, 58, 46, 39, 58]), target: 1\n",
      "context: tensor([52, 58,  1, 58, 46, 39, 58,  1]), target: 46\n",
      "context: tensor([25]), target: 17\n",
      "context: tensor([25, 17]), target: 27\n",
      "context: tensor([25, 17, 27]), target: 10\n",
      "context: tensor([25, 17, 27, 10]), target: 0\n",
      "context: tensor([25, 17, 27, 10,  0]), target: 21\n",
      "context: tensor([25, 17, 27, 10,  0, 21]), target: 1\n",
      "context: tensor([25, 17, 27, 10,  0, 21,  1]), target: 54\n",
      "context: tensor([25, 17, 27, 10,  0, 21,  1, 54]), target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# based on batch and block sizes, generate small batch of data with inputs x and targets y\n",
    "def get_batch(split):\n",
    "    # condition the data to be batched based on whether we're training or validating the model\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    '''\n",
    "     remember that we want to batch the data at random---this helps the learning process\n",
    "     we want the index to start at some random index in the training data,\n",
    "     but we don't want it to overextend the data set\n",
    "     size=(batch_size,) specifies a 1D tensor of size batch_size, it's just the notation \n",
    "     we want it to be batch_size since that's how many batches we want\n",
    "     then the block_size wiill be used as a jump off point\n",
    "    '''\n",
    "    ix = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,))\n",
    "    '''\n",
    "     we want to be able to form the batches cleanly from ix\n",
    "     one way to do this is to create the inputs from ix using torch.stack\n",
    "     to make a sequence of tensors---exactly what i'm looking for\n",
    "    '''\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # we do the same thing to form the ideal outputs of the model\n",
    "    # i.e. the ideal posterior\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# now we get the batches\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# let's see their contents\n",
    "\n",
    "print('------')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('------')\n",
    "\n",
    "# and now let's understand how the model will be learning the inputs in the same way\n",
    "# we'll be iterating through each batch of the model\n",
    "for b in range(batch_size):\n",
    "    # then we'll be iterating through each element in the block\n",
    "    # this is called the 'time' dimension because we're considering each block\n",
    "    # of a batch as elements in a time series\n",
    "    for t in range(block_size):\n",
    "        # then we form the context within the batch\n",
    "        # if we don't add the +1, bc we start at 0th index, we're technically\n",
    "        # not doing next token prediction. Additionally we'd also be starting at 0\n",
    "        context = xb[b, :t+1]\n",
    "        \n",
    "        # the targets are already shifted from the batch creation,\n",
    "        # so no need to shift when iterating through the time block\n",
    "        target = yb[b, t]\n",
    "        print(f'context: {context}, target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35bb3e",
   "metadata": {},
   "source": [
    "### Step 3: Create a Bigram language model\n",
    "\n",
    "#### 3.1: Method 1 - Using only a feed forward approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e39be91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLER\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "'''\n",
    "    Always instantiate model as nn.Module so that PyTorch can cleanly interact with the\n",
    "    features of your model construction. BLM will be a subclass of nn.Module\n",
    "    - allows for creating nn layers easier\n",
    "    - the nn.Module knows what to do when you create a forward() fcn, for example\n",
    "    \n",
    "    __init__(self, vocab_size): where we define layers and operations\n",
    "    params: \n",
    "    - super().__init__(): Include this the majority of the time so that we inherit \n",
    "    everything from the nn.Module class\n",
    "    \n",
    "    - vocab_size: size of vocab list in dataset. We use it to make an embedding table so\n",
    "    that we can reduce the number of dimensions that our data set will be stored in\n",
    "    Alternative approaches include one hot vectors, but those aren't very efficient \n",
    "    for training. Otherwise we'd need to keep track of all possible combos of the bigram\n",
    "    data set. And this scales very well for > 50,000 data sets\n",
    "    --> What you instead do is instantiate a weight matrix, and then pluck out\n",
    "    the corresponding row of that character (which is our token)\n",
    "    \n",
    "    --\n",
    "    fcn: forward(self, idx, targets=None)\n",
    "    parameters: \n",
    "    idx: indices of the inputs, i.e. our tokens. this will be our batch of x\n",
    "    targets: our desired outputs of the BLM\n",
    "''' \n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__() \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        ''' whats going on in the forward pass?\n",
    "        \n",
    "        logits are the result of plucking out the rows of the token emb table\n",
    "        which correspond to the indices of the inputs\n",
    "        -- dim = Batch, Time, Channels = 4, 8, vocab_size=65\n",
    "        ---- these correspond to the scores for the next token to be predicted\n",
    "        logits[0] corresponds to the first batch, where there are 8 vectors which each\n",
    "        correspond to the scores of each of the 65 possible tokens for each \n",
    "        \"moment of time\" which is only 8 or this time. It really corresponds\n",
    "        to the block size because we've discretized time series into blocks\n",
    "        '''\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        \n",
    "        # need to have a test case for when no targets as passed for the generate function\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # we need to reshape logits so that we can compute cross entropy correctly\n",
    "            # we'll call B*T the minibatch dimension\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            \n",
    "            # targets needs to be the minibatch dim according to PyTorch docs\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            # then compute the loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    '''\n",
    "        idx serves as the current context of the model. the job of generate is to \n",
    "        create max_new_tokens more tokens based off this passed context\n",
    "    '''\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # run a for loop which iterates over amount of specified new tokens to generate\n",
    "        for _ in range(max_new_tokens):\n",
    "            # based off of this current context, we want to get the logits but not the loss\n",
    "            # this is why we needed this condition set in the forward pass\n",
    "            logits, loss = self(idx)\n",
    "            \n",
    "            # as we're generating the sequences, we're going to be adding the next\n",
    "            # token to the context, so we'll always want to look to the last dimension\n",
    "            # of the logits in order to calculate the probabilities of predicting\n",
    "            # the next token\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "                        \n",
    "            # apply softmax to get the probabilities\n",
    "            # we want probabilities of the channel dim, since this is the prob\n",
    "            # of a character appearing\n",
    "            probs = F.softmax(logits, dim=-1) # dim = (B, C)\n",
    "                        \n",
    "            # sample from the distribution using multinomial\n",
    "            # binomial -- Y/N, i.e. sampling from distribution of two categories\n",
    "            # multinomial samples from probability distributions of k categories\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to a running sequence of indices throughout gen process\n",
    "            # we append idx_next to the time dimension, so dim=1 since idx dim = (B, T)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # new dim of idx = (B, T+1)\n",
    "            \n",
    "        return idx\n",
    "        \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "# our baby starting point is from a batch=1, time=1 tensor\n",
    "# dtype is a 64bit integer, i.e. long\n",
    "starting_point = torch.zeros((1, 1), dtype=torch.long)\n",
    "decode(m.generate(starting_point, max_new_tokens=50)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ee497",
   "metadata": {},
   "source": [
    "### 3.2 Now let's train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213d1e6",
   "metadata": {},
   "source": [
    "#### 3.2.1. A common choice to make is the Adam optimizer. But we'll be using the weight decay version of the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16363cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's nice about nn.Module is that when instantiating the optimizer in PyTorch,\n",
    "# you can just pass in m.parameters() and it'll auto create them for you\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf52da6",
   "metadata": {},
   "source": [
    "And now we train on a batch size of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a371973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.3 s, sys: 27.6 ms, total: 47.3 s\n",
      "Wall time: 7.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch_size = 32\n",
    "\n",
    "for steps in range(10_000):\n",
    "    # get the batchs from get_batch\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # obtain logits and loss from the created BLM\n",
    "    logits, loss = m(xb, yb)\n",
    "    \n",
    "    # zero out the gradients before doing the backward pass so you don't have an \n",
    "    # accumulation of gradients. \n",
    "    # we don't want accumulation because it will ruin the training process\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # then do backward pass according to the AdamW optimizer\n",
    "    loss.backward()\n",
    "    \n",
    "    # then with this backward pass, update the model\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5268497f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3164, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc189f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whmer thoun s's:Conchuntilalllevise sthat dy hangilyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprroutherc\n"
     ]
    }
   ],
   "source": [
    "starting_point = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(starting_point, max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7b85ec",
   "metadata": {},
   "source": [
    "### 4. Understanding self attention from the ground up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded5f61",
   "metadata": {},
   "source": [
    "#### 4.1. How do we constrain the context window? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb311a",
   "metadata": {},
   "source": [
    "These models should not have access to future time series data. Otherwise that defeats the purpose of training a model to be able to predict. \n",
    "\n",
    "We're going to work with a really basic example of predicting the next token and work our way up to self attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "575ae7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "'''\n",
    "B = batch, which is the batch size = 4\n",
    "T = time, the number of tokens we have in our time series = 8\n",
    "C = channels, i.e. the vocab size, which is just two in this case\n",
    "'''\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C) # how we're embedding the 'inputs' of the model\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea373a",
   "metadata": {},
   "source": [
    "\n",
    "A common way to be able to predict the next token is to average over all previous tokens that the model has previously seen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df72cdf",
   "metadata": {},
   "source": [
    "##### 4.1.1. The most brute force way to average over the previous context to predict the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d573380c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.94 ms, sys: 0 ns, total: 1.94 ms\n",
      "Wall time: 1.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# instantiate our bag of words, i.e. what the model has seen previously\n",
    "# we'll be averaging over this\n",
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "# then we'll brute force iterate through each batch and time in order to take the average\n",
    "# and predict the next token\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        # create vector representing all previous tokens\n",
    "        # need the plus one so that you don't start predicting from an empty vector\n",
    "        xprev = x[b, :t+1]\n",
    "        \n",
    "        # now bag of words will contain the average of each word \n",
    "        # as we continue to see context\n",
    "        #print(xprev)\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)\n",
    "        #print(xbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cb5de67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0341,  0.1332])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f1049a",
   "metadata": {},
   "source": [
    "So in this batch, the last value in xbow represents the average over all of the previous tokens.\n",
    "\n",
    "Now this is all well for getting the averages, but it's not good for efficiency especially as models scale up. We need to do things differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd85ea",
   "metadata": {},
   "source": [
    "##### 4.1.2. Using triangular matrices to represent context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f11cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "----------\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----------\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# the below allows you to more easily see where each element of c comes from\n",
    "#a = torch.tensor(((1., 1., 1.), (2., 2., 2.), (3., 3., 3.)))\n",
    "#a = torch.ones(3, 3) # the standard a, without pulling out the triangular matrix\n",
    "a = torch.tril(torch.ones(3, 3)) # take the triangular matrix of a matrix of all 1s\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('----------')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('----------')\n",
    "print('c=')\n",
    "print(c)\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93df8a",
   "metadata": {},
   "source": [
    "We can use this trick for taking an average over all of our weights that we would be using for training. Like a hybrid of looking at the weighted average of the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "565ac156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "CPU times: user 3.16 ms, sys: 890 µs, total: 4.05 ms\n",
      "Wall time: 777 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# dim(weights) needs to be (T, T) since we are weighting each observation\n",
    "# in the time series\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "\n",
    "'''\n",
    "    In order to normalize the row how we expect, we need to have keepdim=True\n",
    "    because otherwise the tensor broadcasting will not work accordingly\n",
    "    \n",
    "    If you don't specify keepdim=True, PyTorch will make up the dimensions on the left\n",
    "    so that we end up normalizing the columns even though we say dim=1\n",
    "'''\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)\n",
    "print(wei.shape) # dim = (T, T), and now we're averaging\n",
    "\n",
    "# and we create our new bag of words with this trick\n",
    "# (T, T) @ (B, T, C) ==> (B, T, T) @ (B, T, C) = (B, T, C)\n",
    "xbow2 = wei @ x\n",
    "xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc9404ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000, 1.5000, 1.8333, 2.0833, 2.2833, 2.4500, 2.5929, 2.7179]),\n",
       " tensor([[1.0000],\n",
       "         [1.5000],\n",
       "         [1.8333],\n",
       "         [2.0833],\n",
       "         [2.2833],\n",
       "         [2.4500],\n",
       "         [2.5929],\n",
       "         [2.7179]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(wei, dim=1), torch.sum(wei, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c444aa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8]), torch.Size([8, 1]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(wei, dim=1).shape, torch.sum(wei, dim=1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ca81e",
   "metadata": {},
   "source": [
    " Now xbow2 isn't normalized or anything, so the averaging is a little weird. \n",
    " \n",
    " Let's implement this with softmax instead, which is the exponential averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1fabb9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 ms, sys: 7 µs, total: 1.25 ms\n",
      "Wall time: 743 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create the traingular matrix and weights as before, this time let weights\n",
    "# start at zero for better results--we also do a similar thing for sentence generation\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "\n",
    "# now e^(-inf) = 1/e^(inf) = 0, so we need to reinstantiate the weights with this\n",
    "# so that the softmax will work properly\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "# we want to softmax on the rows of the weights\n",
    "wei = F.softmax(wei, dim=1)\n",
    "# now that we have our normalized weights, we need to multiply as before\n",
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c25e21",
   "metadata": {},
   "source": [
    "And now we have trainable weights which are siphoned off from a context at a given point in the observation window. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436eff95",
   "metadata": {},
   "source": [
    "##### 4.1.4. Now onto actual self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af41b41",
   "metadata": {},
   "source": [
    "Self attention intends to be a \"smarter\" averaging. We still want to average over the past, but if something is more relevant for the next predicted token, we want this to flow up to the surface. This is what self attention intends to solve. I am not sure exactly how this is done yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23f17e",
   "metadata": {},
   "source": [
    "We are going to characterize each token with two feature vectors: Queries and keys. \n",
    "\n",
    "**Queries**: What am I looking for\n",
    "\n",
    "**Keys**: What do I contain\n",
    "\n",
    "For example let's look at the following sentence: \n",
    "\n",
    "All that glitters is not gold\n",
    "\n",
    "When we're looking at \"glitters\", we want to hope that \"gold\" is relevant enough to have a high probability of appearing within the next token prediction. When we're actually doing the learning process, we're taking the 'query' vector of \"glitters\" and we're going to dot product with all of the other words (tokens) in the sentence, which will be the \"values\". This dot product will create a score. That score will indicate the likelihood that that token will appear next in the sentence. We will be training the model so that the query+key and value vectors will be independently tuned.\n",
    "\n",
    "This score is really the attention score. The \"self\" attention part comes from how all of these vectors are from the inputs themselves. \n",
    "\n",
    "Then the \"values\" vector of self attention will further provide an actual context dependence to the score\". Since for instance, gold isn't the only thing that glitters. Diamonds also glitter. So once the query/key vectors are dot producted and softmax'd, the values will further contribute to the attention scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cd2dd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# create a single Head of self attention\n",
    "# value is the \"public\" version of the values of x, so we don't have to directly matmul it\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # (dim_input, dim_output, bias)\n",
    "query = nn.Linear(C, head_size, bias=False) # (dim_input, dim_output, bias)\n",
    "value = nn.Linear(C, head_size, bias=False) # (dim_input, dim_output, bias)\n",
    "\n",
    "# performs x . key^T or x . query^T since there is no bias\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "# do batch matrix multiplication---the last two dimensions will multiply like regular matrices\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T), the last two dimensions do regular matmul\n",
    "\n",
    "# and now we do the same thing as before\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325622a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
