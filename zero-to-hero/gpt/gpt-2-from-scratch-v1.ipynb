{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5129ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-29 12:55:37--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.9’\n",
      "\n",
      "input.txt.9         100%[===================>]   1.06M  2.43MB/s    in 0.4s    \n",
      "\n",
      "2025-08-29 12:55:38 (2.43 MB/s) - ‘input.txt.9’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b730f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6150c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c360d368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c858a5",
   "metadata": {},
   "source": [
    "Tokenize: Map characters ==> sequence of integers according to some vocabulary of possiblilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9738d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: takes in a string, outputs integer list\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # input integer list, output string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f95328",
   "metadata": {},
   "source": [
    "Now let's tokenize the entire tinyshakespeare data set with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e27ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# then let's split the data into training and validation data\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779df5dd",
   "metadata": {},
   "source": [
    "It is computationally expensive to feed all of the data into the transformers at once. A way to navigate this is to use a context length or block size to chunk the incoming data so the transformer can learn it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e773b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc51830",
   "metadata": {},
   "source": [
    " Then the transformer will basically enumerate and learn which integers will come after another in a combinatorial way. Itll look at 18 and 47 and predict that 56 should come next. Then it will look at 18 47 56 and know that 57 should come next. And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537ac64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1] # attempting to predict what's going to immediately follow the block size?\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context} the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a961b",
   "metadata": {},
   "source": [
    "Now we will form the input data as batches according to how many rows of a block we want to give."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a545618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 76049, 234249, 934904, 560986])\n",
      "x: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "------\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "------\n",
      "when input is [24] the target is 43\n",
      "when input is [24, 43] the target is 58\n",
      "when input is [24, 43, 58] the target is 5\n",
      "when input is [24, 43, 58, 5] the target is 57\n",
      "when input is [24, 43, 58, 5, 57] the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "when input is [44] the target is 53\n",
      "when input is [44, 53] the target is 56\n",
      "when input is [44, 53, 56] the target is 1\n",
      "when input is [44, 53, 56, 1] the target is 58\n",
      "when input is [44, 53, 56, 1, 58] the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52] the target is 58\n",
      "when input is [52, 58] the target is 1\n",
      "when input is [52, 58, 1] the target is 58\n",
      "when input is [52, 58, 1, 58] the target is 46\n",
      "when input is [52, 58, 1, 58, 46] the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "when input is [25] the target is 17\n",
      "when input is [25, 17] the target is 27\n",
      "when input is [25, 17, 27] the target is 10\n",
      "when input is [25, 17, 27, 10] the target is 0\n",
      "when input is [25, 17, 27, 10, 0] the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "# based on batch and block sizes, generate small batch of data with inputs x and targets y\n",
    "def get_batch(split):\n",
    "    # data is training data if specified, otherwise it's validation\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # randomly pull (starting points?) in data at random,\n",
    "    # find the first (block_size) characters that follow it\n",
    "    # and do four of these because the batch size is four\n",
    "    # need to subtract off block size so you don't try to extend over the size of the data\n",
    "    ix = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    print(ix)\n",
    "    \n",
    "    # craft the inpput data based on the block size\n",
    "    # and we want to concatenate all of these blocks into a nice batched tensor\n",
    "    # so we use torch.stack to do this\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    print(f'x: {x}')\n",
    "    \n",
    "    # then we always want to predict whatever character is going to come after the \n",
    "    # iteration of the block size, so we add one for make the target data\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('------')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('------')\n",
    "\n",
    "for b in range(batch_size): # iterating along the batch dimension\n",
    "    for t in range(block_size): # iterating along the block\n",
    "        context = xb[b, :t+1] # add one because we start at zero\n",
    "        target = yb[b, t] # y is already shifted so no need to add anything here\n",
    "        print(f'when input is {context.tolist()} the target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352df87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # this is our input into a transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d58ff",
   "metadata": {},
   "source": [
    "Now we're going to set up a simple bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b1edf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "logits.shape: torch.Size([1, 1, 65])\n",
      "logits.shape: torch.Size([1, 2, 65])\n",
      "logits.shape: torch.Size([1, 3, 65])\n",
      "logits.shape: torch.Size([1, 4, 65])\n",
      "logits.shape: torch.Size([1, 5, 65])\n",
      "logits.shape: torch.Size([1, 6, 65])\n",
      "logits.shape: torch.Size([1, 7, 65])\n",
      "logits.shape: torch.Size([1, 8, 65])\n",
      "logits.shape: torch.Size([1, 9, 65])\n",
      "logits.shape: torch.Size([1, 10, 65])\n",
      "logits.shape: torch.Size([1, 11, 65])\n",
      "logits.shape: torch.Size([1, 12, 65])\n",
      "logits.shape: torch.Size([1, 13, 65])\n",
      "logits.shape: torch.Size([1, 14, 65])\n",
      "logits.shape: torch.Size([1, 15, 65])\n",
      "logits.shape: torch.Size([1, 16, 65])\n",
      "logits.shape: torch.Size([1, 17, 65])\n",
      "logits.shape: torch.Size([1, 18, 65])\n",
      "logits.shape: torch.Size([1, 19, 65])\n",
      "logits.shape: torch.Size([1, 20, 65])\n",
      "logits.shape: torch.Size([1, 21, 65])\n",
      "logits.shape: torch.Size([1, 22, 65])\n",
      "logits.shape: torch.Size([1, 23, 65])\n",
      "logits.shape: torch.Size([1, 24, 65])\n",
      "logits.shape: torch.Size([1, 25, 65])\n",
      "logits.shape: torch.Size([1, 26, 65])\n",
      "logits.shape: torch.Size([1, 27, 65])\n",
      "logits.shape: torch.Size([1, 28, 65])\n",
      "logits.shape: torch.Size([1, 29, 65])\n",
      "logits.shape: torch.Size([1, 30, 65])\n",
      "logits.shape: torch.Size([1, 31, 65])\n",
      "logits.shape: torch.Size([1, 32, 65])\n",
      "logits.shape: torch.Size([1, 33, 65])\n",
      "logits.shape: torch.Size([1, 34, 65])\n",
      "logits.shape: torch.Size([1, 35, 65])\n",
      "logits.shape: torch.Size([1, 36, 65])\n",
      "logits.shape: torch.Size([1, 37, 65])\n",
      "logits.shape: torch.Size([1, 38, 65])\n",
      "logits.shape: torch.Size([1, 39, 65])\n",
      "logits.shape: torch.Size([1, 40, 65])\n",
      "logits.shape: torch.Size([1, 41, 65])\n",
      "logits.shape: torch.Size([1, 42, 65])\n",
      "logits.shape: torch.Size([1, 43, 65])\n",
      "logits.shape: torch.Size([1, 44, 65])\n",
      "logits.shape: torch.Size([1, 45, 65])\n",
      "logits.shape: torch.Size([1, 46, 65])\n",
      "logits.shape: torch.Size([1, 47, 65])\n",
      "logits.shape: torch.Size([1, 48, 65])\n",
      "logits.shape: torch.Size([1, 49, 65])\n",
      "logits.shape: torch.Size([1, 50, 65])\n",
      "logits.shape: torch.Size([1, 51, 65])\n",
      "logits.shape: torch.Size([1, 52, 65])\n",
      "logits.shape: torch.Size([1, 53, 65])\n",
      "logits.shape: torch.Size([1, 54, 65])\n",
      "logits.shape: torch.Size([1, 55, 65])\n",
      "logits.shape: torch.Size([1, 56, 65])\n",
      "logits.shape: torch.Size([1, 57, 65])\n",
      "logits.shape: torch.Size([1, 58, 65])\n",
      "logits.shape: torch.Size([1, 59, 65])\n",
      "logits.shape: torch.Size([1, 60, 65])\n",
      "logits.shape: torch.Size([1, 61, 65])\n",
      "logits.shape: torch.Size([1, 62, 65])\n",
      "logits.shape: torch.Size([1, 63, 65])\n",
      "logits.shape: torch.Size([1, 64, 65])\n",
      "logits.shape: torch.Size([1, 65, 65])\n",
      "logits.shape: torch.Size([1, 66, 65])\n",
      "logits.shape: torch.Size([1, 67, 65])\n",
      "logits.shape: torch.Size([1, 68, 65])\n",
      "logits.shape: torch.Size([1, 69, 65])\n",
      "logits.shape: torch.Size([1, 70, 65])\n",
      "logits.shape: torch.Size([1, 71, 65])\n",
      "logits.shape: torch.Size([1, 72, 65])\n",
      "logits.shape: torch.Size([1, 73, 65])\n",
      "logits.shape: torch.Size([1, 74, 65])\n",
      "logits.shape: torch.Size([1, 75, 65])\n",
      "logits.shape: torch.Size([1, 76, 65])\n",
      "logits.shape: torch.Size([1, 77, 65])\n",
      "logits.shape: torch.Size([1, 78, 65])\n",
      "logits.shape: torch.Size([1, 79, 65])\n",
      "logits.shape: torch.Size([1, 80, 65])\n",
      "logits.shape: torch.Size([1, 81, 65])\n",
      "logits.shape: torch.Size([1, 82, 65])\n",
      "logits.shape: torch.Size([1, 83, 65])\n",
      "logits.shape: torch.Size([1, 84, 65])\n",
      "logits.shape: torch.Size([1, 85, 65])\n",
      "logits.shape: torch.Size([1, 86, 65])\n",
      "logits.shape: torch.Size([1, 87, 65])\n",
      "logits.shape: torch.Size([1, 88, 65])\n",
      "logits.shape: torch.Size([1, 89, 65])\n",
      "logits.shape: torch.Size([1, 90, 65])\n",
      "logits.shape: torch.Size([1, 91, 65])\n",
      "logits.shape: torch.Size([1, 92, 65])\n",
      "logits.shape: torch.Size([1, 93, 65])\n",
      "logits.shape: torch.Size([1, 94, 65])\n",
      "logits.shape: torch.Size([1, 95, 65])\n",
      "logits.shape: torch.Size([1, 96, 65])\n",
      "logits.shape: torch.Size([1, 97, 65])\n",
      "logits.shape: torch.Size([1, 98, 65])\n",
      "logits.shape: torch.Size([1, 99, 65])\n",
      "logits.shape: torch.Size([1, 100, 65])\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# here there are four batches, block_size is 8, channel is the vocab size\n",
    "# logits are the scores for the next characters of the sequence, which \n",
    "# will go to the token embedding table and pluck out the row corresponding\n",
    "# to the idx, which is really each xb\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token will pluc\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    # (batch, time, channel) = (4, 8, 65)\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (batch, time, channel)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None            \n",
    "        else:\n",
    "            # need to reshape logits so that it fits into cross entropy functional form\n",
    "            # bc pytorch expects you to pass in your logits this way\n",
    "            B, T, C = logits.shape\n",
    "            #print(f'B, T, C: {B}, {T}, {C}')\n",
    "            logits = logits.view(B*T, C)\n",
    "\n",
    "            # need to do the same as targets\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    # now let's generate from the trained model\n",
    "    # idx is the current context of some characters in a batch -- (B, T)\n",
    "    # generate extends it to be (B, T+1), then again to (B, T+2) ...\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            print(f'logits.shape: {logits.shape}')\n",
    "            \n",
    "            # for now we'll only focus on the last time step (last part of the block)\n",
    "            # since the predications are following whatever was there previously\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "\n",
    "            \n",
    "            # apply softmax to get the probabilities\n",
    "            # dim=-1 bc we want channel probabilities (?)\n",
    "            probs = F.softmax(logits, dim=0) # dim = (B, C)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            # one sample per batch\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            \n",
    "            # append sampled index to a running sequence of indices throughout gen process\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            \n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# 0 is how we are going to kick off the generation. we're at time step 1\n",
    "#idx = torch.zeros((1, 1), dtype=torch.long\n",
    "\n",
    "# then m.generate will continue the generation for 100 tokens\n",
    "# generate works on the batch level, so we have to index to the 0 element\n",
    "# because we're assuming batch size is 1 here\n",
    "# this is just an example, and could technically start from a different element\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b604320",
   "metadata": {},
   "source": [
    "So we should expect the loss to be completely uniform across the entire vocabulary size distribution, since the probability of each should be $-\\mathrm{ln}(1/65) \\approx 4.17$ but we end up getting $4.87$.. which isn't that good. It's not completely uniform, which means that there's some correlations (?) and some entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b121396",
   "metadata": {},
   "source": [
    "### Checkpoint at 35:00: now let's train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5517b44",
   "metadata": {},
   "source": [
    "For this we'll need an optimizer: One standard optimizer is Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49db876",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3252537",
   "metadata": {},
   "source": [
    "Never forget to zero your gradients out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbcc14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# 100 steps doesn't really optimize too much --- went from 4.7 to 4.58, but we know a lower bound is 4.17\n",
    "# 1_000 got me to 3.63, a couple more times yields \n",
    "for steps in range(10_000):\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bfffdcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7320b",
   "metadata": {},
   "source": [
    "Now onto a mathematical trick that is commonly used in self attention. Begin at 42:00\n",
    "\n",
    "The trick has something to do with context length. The model should not have access to future tokens, and only be able to reference those that it has seen from the past in order to predict the next token.\n",
    "\n",
    "We want to take an average over all of the past history to be able to predict the next token. We're really creating what's called a feature vector to represent this quantity. We'll be working directly with the channels of x in order to make this happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f411bb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf859a1",
   "metadata": {},
   "source": [
    "bow = bag of words, which is a common term for the average of the model's history\n",
    "\n",
    "each xprev is the the input up until some token when specified a certain batch\n",
    "\n",
    "remember that C = channels, which is really represented by the vocabulary size. we're just using c = 2 as an example for now since we want to understand the mathematical trick.\n",
    "\n",
    "so we're looking at each batch. the T dimension is here because this is our time series, and each token is obtained at each point in the observation process. And because our toy vocab size is just 2, we're really averaging over possible token sequences which have two possible characters. but we are starting off with x being initialized with random floats. The idea is that we'd train the model through Adam so the model converges onto something useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bee657a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # we're in some batch b, and looking at all of the previous tokens up to token t. keeping dim C\n",
    "        xbow[b, t] = torch.mean(xprev, 0) # then we'll store the average into xbow as a one dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362b28d",
   "metadata": {},
   "source": [
    "However this is pretty inefficient, and we want to speed things up a bit. \n",
    "\n",
    "To illustrate how we can speed things up, we'll play with some easy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "590a66e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "----------\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "----------\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# the below allows you to more easily see where each element of c comes from\n",
    "#a = torch.tensor(((1., 1., 1.), (2., 2., 2.), (3., 3., 3.)))\n",
    "#a = torch.ones(3, 3) # the standard a, without pulling out the triangular matrix\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('----------')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('----------')\n",
    "print('c=')\n",
    "print(c)\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31153c5",
   "metadata": {},
   "source": [
    "Now let's implement this with our weights and create a new xbow, xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dea84fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "'''\n",
    "At first the following matrix multiplication will involve\n",
    "(T, T) @ (B, T, C)\n",
    "but PyTorch will recognize that the tensor broadcasting will not work\n",
    "so it always creates the dimension on the left; in this case, it's the batch dimension\n",
    "==> (B, T, T) @ (B, T, C) then becomes (B, T, C) = xbow2\n",
    "'''\n",
    "xbow2 = wei @ x \n",
    "torch.allclose(xbow, xbow2, rtol=1e-05, atol=1e-07, equal_nan=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4308241",
   "metadata": {},
   "source": [
    "And now for method #3, which will utilize softmax to create the weights. this is a method of setting up the context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74883f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T)) # create the triangular matrix\n",
    "wei = torch.zeros((T, T)) # instantiate weights\n",
    "wei = wei.masked_fill(transformer_from_scratch.pyil == 0, float('-inf')) # create what'll be put into softmax\n",
    "wei = F.softmax(wei, dim=-1) # softmax on every row\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba629d7",
   "metadata": {},
   "source": [
    "Question for comp mech-y implementations: Could we replace self attention with causal states? If so, how better can we predict the next token?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b7193",
   "metadata": {},
   "source": [
    "#### Full self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2240e",
   "metadata": {},
   "source": [
    "And now onto version 4, where we'll be implementing the full version of self attention\n",
    "\n",
    "Now at least for the bigram model, for example if a vowel was the previously seen token then we are way more likely to look for a consonant. So even though we are averaging the past contexts, we want to funnel the past information which is specifically relevent to the present. This is the problem that self attention solves\n",
    "\n",
    "How it works is that each token (going to stop here because I feel like I'm going at 80% and getting more distracted. Currently at 1:02:00. Also my bigram.py doesn't work I haven't tried to figure out why yet, and I'm hoping it'll get resolved once we implement self attention into the bigram model)\n",
    "\n",
    "Alright, now onto the full version of self attention\n",
    "\n",
    "Each token will have two characterizing vectors: Key and query. \n",
    "\n",
    "**Query:** What am I looking for\n",
    "\n",
    "**Key:** What do I contain\n",
    "\n",
    "Dot products with queries and keys will now become weights, and the infinities will show up. This will increase likely hood of high probability tokens interacting with each other, and decrease interactions between low probability past tokens\n",
    "\n",
    "To me right now, keys and queries are basically the same. They're both linear layers and don't have any differences between them right now besides the random numbers that they'll contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e3d0041",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# create a single Head of self attention\n",
    "# value is the \"public\" version of the values of x, so we don't have to directly matmul it\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # (dim_input, dim_output, bias)\n",
    "query = nn.Linear(C, head_size, bias=False) # (dim_input, dim_output, bias)\n",
    "value = nn.Linear(C, head_size, bias=False) # (dim_input, dim_output, bias)\n",
    "\n",
    "# performs x . key^T or x . query^T since there is no bias\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "# do batch matrix multiplication---the last two dimensions will multiply like regular matrices\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T), the last two dimensions do regular matmul\n",
    "\n",
    "# and now we do the same thing as before\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f73d6026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
       "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
       "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5579eb",
   "metadata": {},
   "source": [
    "Now we've created data-dependent batch elements that we can optimize on. The higher probabilities in the tokens prior to the present token will have greater probability than those that aren't. Of course when we're starting out, this is just randomized. But as we train, the probabilities depending on whether the current token was a vowel or a consonant will be tuned accordingly. \n",
    "\n",
    "Attention is a communication system because it aggregates past information into vector information, which allows data dependent inference on past tokens in order to predict the next token. At any given node, all other nodes are pointing directly to it, and depending on their dependence, the arrows connecting the nodes will contribute more to the weighted sum\n",
    "\n",
    "There is also no notion of space in the self attention head, there's really only the vectors being multipled by each other. \n",
    "\n",
    "Batching the data allows them to be processed in parallel and independently\n",
    "\n",
    "We're building a decoder because we have an autoregressive way of masking the triangular matrix to ensure good context. You can delete the masking in order to have an \"encoding\" structure which is useful for things like analyzing the sentiment of a sentence. In that case you'd want all nodes to talk to each other and note just the past tokens. But we're instantiating a position dependence in the nodes through the triangular matrix in order to predict the next token\n",
    "\n",
    "It is called self attention because the keys, values and queries all come from x. The same source. In encoder-decoder transformers, you can have keys and values coming from one source but queries are coming from another place. This intermixing allows for more complicated? learning behaviors\n",
    "\n",
    "Now if we don't divide by the square root of the head size, then the variance will be on the order of the head size. We don't want this because the variance isn't preserved. The inputs are initially random Gaussian, so this division will keep the structure of the input. \n",
    "\n",
    "The inputs to softmax will converge to one hot vectors if the inputs (weights) are either too negative or positive. We want diffuse weights during the learning process, so we don't want sparse weights. I just tried this and indeed it makes even this case more diffuse than it originally was"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf141b8",
   "metadata": {},
   "source": [
    "Current questions: \n",
    "1) I'm not seeing how either the different keys and queries are communicating with each other within this directed graph intuition picture. I know that the weights are going to be optimized. Well actually the weights are a dot product of the queries and keys, so that means we'll be optimizing the queries and keys as well. This means that based on some next token in the training data, we'll be adjusting the weights so that a certain letter will follow another letter, depending on what's likely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23599fe2",
   "metadata": {},
   "source": [
    "Now we're going to move onto the blocks in the transformer. We're going to be replicating and interspersing the (encoder-decoder?) blocks and then having them repeat so that the training is being iterated on multiple times. (Is this like an RNN kind of thing?) Currently at 1:27:00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff942a58",
   "metadata": {},
   "source": [
    "Now we're going to implement LayerNorm, which is another innovation that is added to the transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b8127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we normalize the rows instead of the columns of the vectors in each batch\n",
    "class LayerNorm1d:\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # calculate the forward pass\n",
    "        xmean = x.mean(1, keepdim=True) # batch mean \n",
    "        xvar = x.var(1, keepdim=True) # batch variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size of 32, with 100 dimensional vectors\n",
    "x = module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "278a011c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803), tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0].mean(), x[:, 0].std(), x[0, :].mean(), x[0, :].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a60fd",
   "metadata": {},
   "source": [
    "Currently at 1:35:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a17eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d33f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f43c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
