{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d9c7de",
   "metadata": {},
   "source": [
    "Making this a part 4, since Andrej seems like we're going to eat our dessert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83da212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd4aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Value(data={self.data})'\n",
    "\n",
    "    # addition\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        # back propogation specifically for the addition\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        # writing the line below *saves* the function into out._backward as the _backward function\n",
    "        # and we are not calling the function itself!\n",
    "        # you'll get errors if you write out._backward = _backward(), since we're not trying to save\n",
    "        # a function call, but a function itself\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # negation, which will be used for subtraction\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    # subtraction--the addition of the negation of something else\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    # multiplication\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        # back propogation specifically for multiplication\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # anything raised to any power by the power rule\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), 'only suppporting ints and floats for now'\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (other*self.data**(other-1)) * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # true division for the constant k = -1 in (a/b) = a*b**-1\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    # since python doesn't know the difference b/n e.g. 2 * a and a * 2\n",
    "    # need a rescue function that'll save a failure from happening by reversing\n",
    "    # order of multiplication\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "    \n",
    "    # temporary activation function\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            # local derivative of e^x was just calculated for out\n",
    "            # multiply this by out.grad for chen lu\n",
    "            self.grad += out.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # backpropogation to be performed given neuron connections\n",
    "    def backward(self):\n",
    "        # first need to topologicall sort connected neurons\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        \n",
    "        # build the topological sort to prepare for backpropogation\n",
    "        build_topo(self)\n",
    "\n",
    "        # now we want the back propogation to happen after the sort\n",
    "        self.grad = 1.0 # since gradient of last neuron will be 1\n",
    "        for node in reversed(topo):\n",
    "            # will call whatever backprop method is needed from other methods\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fa15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # construct graph with nodes and connected edges\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    \n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        \n",
    "        # for every value in the graph, create a node for it\n",
    "        dot.node(name = uid, label='{data = %.2f | grad %.2f}' % (n.data, n.grad), shape='record')\n",
    "        \n",
    "        # if the current node is an operation, create a node for it\n",
    "        if n._op:\n",
    "            dot.node(name=uid + n._op, label=n._op)\n",
    "            dot.edge(uid + n._op, uid)\n",
    "            \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f34933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    # constructor function\n",
    "    # takes in number of inputs: nin\n",
    "    # weights are randomly initialized, and assigned with the number of inputs there are\n",
    "    # bias is also initialized to be some random number\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1.0, 1.0)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1.0, 1.0))\n",
    "        \n",
    "    # call function -- allows you to use an object's method as if it were a function\n",
    "    def __call__(self, x):\n",
    "        # we want sum(wi * xi) + b for all of the weights and inputs\n",
    "        # eventually to feed into activation function    \n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    # we want a way to be able to access the parameters of the network\n",
    "    # define this in order to act like PyTorch and call for them\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "\n",
    "# to make a layer of neurons, specify how many inputs + outputs there are\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        # layer of neurons is literally a list of neurons\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # outputs will be the activated neuron signals from the layer of neurons\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    # let's call the parameters for the layer itself using list comprehension\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "    \n",
    "# now we're making a layer of layers -- a multilayer perceptron (MLP)\n",
    "class MLP:\n",
    "    # nin is the number of inputs\n",
    "    # nouts specifies the number of neurons that there will be \n",
    "    # ex. [4, 4, 1] specifies four neurons in second and third layers, and one neuron serves as the output layer\n",
    "    # of the MLP\n",
    "    def __init__(self, nin, nouts):\n",
    "        # size of the MLP, basically specifies neuron geometry\n",
    "        sz = [nin] + nouts \n",
    "        # construct layers by (number of neurons, number of outconnecting neurons)\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # iterates through all of the layers in order to obtain an output\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    # be able to call parameters of the MLP like how PyTorch does\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "624c3857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.31360688232152)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf4cb3",
   "metadata": {},
   "source": [
    "We're going to make a training loop so we don't have to do this by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83e488f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input data set\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "# specify how our neural net should classify these inputs\n",
    "# these are the targets of the neural network, they are the desired Bayseian posterior (?)\n",
    "# given any of the list of inputs, we want the output to match with the element of each target\n",
    "ys = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64844c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.06417028402021124\n",
      "1 0.06314234263677111\n",
      "2 0.06214414888920264\n",
      "3 0.06117448560552535\n",
      "4 0.06023219912515967\n",
      "5 0.059316195308163996\n",
      "6 0.05842543583557702\n",
      "7 0.057558934776833576\n",
      "8 0.056715755402431886\n",
      "9 0.05589500722200426\n",
      "10 0.055095843229732\n",
      "11 0.05431745734065357\n",
      "12 0.05355908200286563\n",
      "13 0.052819985971927184\n",
      "14 0.05209947223496669\n",
      "15 0.05139687607305968\n",
      "16 0.050711563251420386\n",
      "17 0.05004292832783105\n",
      "18 0.04939039307053075\n",
      "19 0.048753404977514785\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "\n",
    "    # prior to backpropogation, you have to zero your grads\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    # do backpropogation after calculating the loss\n",
    "    loss.backward()\n",
    "    \n",
    "    # now update all of the networks parameters\n",
    "    step_size = 0.01\n",
    "    for p in n.parameters():\n",
    "        p.data += -step_size * p.grad\n",
    "        \n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1e1d8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9037144625783332),\n",
       " Value(data=-0.948509803716335),\n",
       " Value(data=-0.8564824661929197),\n",
       " Value(data=0.8725873733198362)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c69090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
