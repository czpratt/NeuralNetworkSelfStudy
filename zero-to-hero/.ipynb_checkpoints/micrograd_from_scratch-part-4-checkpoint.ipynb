{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d9c7de",
   "metadata": {},
   "source": [
    "Making this a part 4, since Andrej seems like we're going to make the dessert now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83da212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11adf51b",
   "metadata": {},
   "source": [
    "the labelling scheme really screws everything up so I'm going to get rid of it\n",
    "in the actual documentation on Github, there is no label function, \n",
    "so I think it's best to just scrap it, make a copy, and start over\n",
    "my actual draw_dot(n(x)) isn't even accurate at this point anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd4aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Value(data={self.data})'\n",
    "\n",
    "    # addition\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        # back propogation specifically for the addition\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # negation, which will be used for subtraction\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    # subtraction--the addition of the negation of something else\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "    \n",
    "    # multiplication\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        # back propogation specifically for multiplication\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # anything raised to any power by the power rule\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), 'only suppporting ints and floats for now'\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (other*self.data**(other-1)) * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # true division for the constant k = -1 in (a/b) = a*b**-1\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    # since python doesn't know the difference b/n e.g. 2 * a and a * 2\n",
    "    # need a rescue function that'll save a failure from happening by reversing\n",
    "    # order of multiplication\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "    \n",
    "    # temporary activation function\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self, ), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            # local derivative of e^x was just calculated for out\n",
    "            # multiply this by out.grad for chen lu\n",
    "            self.grad += out.data * out.grad\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    # backpropogation to be performed given neuron connections\n",
    "    def backward(self):\n",
    "        # first need to topologicall sort connected neurons\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        \n",
    "        # build the topological sort to prepare for backpropogation\n",
    "        build_topo(self)\n",
    "\n",
    "        # now we want the back propogation to happen after the sort\n",
    "        self.grad = 1.0 # since gradient of last neuron will be 1\n",
    "        for node in reversed(topo):\n",
    "            # will call whatever backprop method is needed from other methods\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fa15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    # construct graph with nodes and connected edges\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    \n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        \n",
    "        # for every value in the graph, create a node for it\n",
    "        dot.node(name = uid, label='{data = %.2f | grad %.2f}' % (n.data, n.grad), shape='record')\n",
    "        \n",
    "        # if the current node is an operation, create a node for it\n",
    "        if n._op:\n",
    "            dot.node(name=uid + n._op, label=n._op)\n",
    "            dot.edge(uid + n._op, uid)\n",
    "            \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "        \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f34933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    # constructor function\n",
    "    # takes in number of inputs: nin\n",
    "    # weights are randomly initialized, and assigned with the number of inputs there are\n",
    "    # bias is also initialized to be some random number\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1.0, 1.0)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1.0, 1.0))\n",
    "        \n",
    "    # call function -- allows you to use an object's method as if it were a function\n",
    "    def __call__(self, x):\n",
    "        # we want sum(wi * xi) + b for all of the weights and inputs\n",
    "        # eventually to feed into activation function    \n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    # we want a way to be able to access the parameters of the network\n",
    "    # define this in order to act like PyTorch and call for them\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "\n",
    "# to make a layer of neurons, specify how many inputs + outputs there are\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        # layer of neurons is literally a list of neurons\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # outputs will be the activated neuron signals from the layer of neurons\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    # let's call the parameters for the layer itself using list comprehension\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "    \n",
    "# now we're making a layer of layers -- a multilayer perceptron (MLP)\n",
    "class MLP:\n",
    "    # nin is the number of inputs\n",
    "    # nouts specifies the number of neurons that there will be \n",
    "    # ex. [4, 4, 1] specifies four neurons in second and third layers, and one neuron serves as the output layer\n",
    "    # of the MLP\n",
    "    def __init__(self, nin, nouts):\n",
    "        # size of the MLP, basically specifies neuron geometry\n",
    "        sz = [nin] + nouts \n",
    "        # construct layers by (number of neurons, number of outconnecting neurons)\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # iterates through all of the layers in order to obtain an output\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    # be able to call parameters of the MLP like how PyTorch does\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "624c3857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.6722296315040197)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac3313e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43c98b",
   "metadata": {},
   "source": [
    "Now let's do a simpler example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0189b4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.6722296315040197),\n",
       " Value(data=-0.3680023033977053),\n",
       " Value(data=-0.8333200601615005),\n",
       " Value(data=-0.8978947742322225)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define input data set\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "# specify how our neural net should classify these inputs\n",
    "# these are the targets of the neural network, they are the desired Bayseian posterior (?)\n",
    "# given any of the list of inputs, we want the output to match with the element of each target\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "# let's just test out what the neural network will output, regardless of how we want to adjust the weights\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d9f84a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=6.825559805393221)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the loss function, which characterizes how far away the neural networks predictions are\n",
    "# to the ground truth\n",
    "loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82f2ca90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.029018594614850755)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = [n(x) for x in xs]\n",
    "loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1798d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do back propogation on the loss as we understand how to minimize the loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3335b049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.6970290339280119"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c799a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4390752746774942"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6351bf76",
   "metadata": {},
   "source": [
    "In gradient descent, the gradient will always point in the direction of increased loss\n",
    "so this means if that the gradient is negative, this will mean that aiming for a negative slope\n",
    "will always increase \n",
    "the loss. but for this binary classification problem, we want to minimize the loss\n",
    "because we are trying to make the predictions of the neural network match up with the target\n",
    "\n",
    "so what this means is that we have to adjust the weight's data with taking the direction\n",
    "of the gradient into account\n",
    "\n",
    "so we are now trying to iterate through the neural network, and adjust the weight's values\n",
    "by some small step size multiplied by the value of the gradient\n",
    "\n",
    "in this example, the gradient is negative, so that means we want to adjust this particular\n",
    "weight by a positive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee499bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.1\n",
    "# iterate through the parameters to adjust the loss\n",
    "\n",
    "for p in n.parameters():\n",
    "    p.data += -step_size * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50d5aab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9998860078181403),\n",
       " Value(data=-0.9908102755993529),\n",
       " Value(data=-0.8298996797992249),\n",
       " Value(data=0.9998920478478914)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd74eb",
   "metadata": {},
   "source": [
    "We have quite literally trained a neural network by hand for the first time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7687ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
